\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-----------------------PAGE SETTINGS----------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[margin=0.01cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%--------------------------PREAMBLE------------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{anyfontsize}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{siunitx}
\usepackage{todonotes}

\definecolor{lightblue}{rgb}{0.23, 0.45, 0.57}
\usepackage[colorlinks=true,linkcolor=blue,allcolors=lightblue]{hyperref}
\usepackage[toc,page]{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-----------------------CUSTOM COMMANDS--------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\textBF}[1]{%
    \pdfliteral direct {2 Tr 1 w} %the second factor is the boldness
     #1%
    \pdfliteral direct {0 Tr 0 w}%
}

\newcommand{\textDF}[1]{%
    \pdfliteral direct {2 Tr 0.2 w} %the second factor is the boldness
     #1%
    \pdfliteral direct {0 Tr 0 w}%
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%-----------------------DOCUMENT BEGIN---------------------------%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{titlepage}
\thispagestyle{empty}
\title{
\includegraphics[width=19cm]{images/rug_fse_cs_logo.pdf} \\
\vspace{3cm}
\begingroup
\setstretch{4}\fontsize{38}{10}\selectfont\fontdimen2\font=0.8ex
\parbox{13.3cm}{\textBF{Evaluating the performance of feature ranking algorithms.}} %%Title
\endgroup}
\date{July 2021}
\author{J.G.S. Overschie}
\maketitle
\vspace{-3.5cm}
\hspace{4cm}\parbox[b][15cm][b]{8cm}{\textDF{\large\setstretch{1.5}
MSc Thesis INMSTAG-08\\
Student: J.G.S. Overschie\\ %NAME
First supervisor: dr. G. Azzopardi\\
Second assessor: A.M.J.A. Alsahaf}}
\end{titlepage}
\newpage
\newgeometry{top=1in,bottom=1in,right=1.25in,left=1.25in} %Needed because margins were changed for titlepage

\hrule
\begin{abstract}
\end{abstract}
\hrule
\begin{quote}
    
\end{quote}

\tableofcontents
\section{Introduction}
% more data: many domains
In this day and age, more data is available than ever before in many domains. In the biomedical domain sensory devices such as MRI or PET scanners are getting ever more accurate - requiring more storage space to store higher resolution data; in the financial domain markets are operating at increasingly low time intervals - requiring storage and analysis of data at a higher time resolution than before and lastly; the internet is increasing in amount of traffic on a global scale and is producing immense amounts of data. Applications of Machine Learning are common in all these domains: training predictive models by learning from example data is able to give us interesting insights that can improve both economy and the quality of human lives. By the nature of models that learn from examples, performance is often better when a larger amount of examples is available. But we shall find out that larger quantities of data presents itself not as solely beneficial; but rather a mixed blessing.

% ML can benefit: but also harder.
The field of Machine Learning has seen vast increases in dataset sizes: both in terms of sample size and amount of dimensions. But whilst the availability of more data presents practitioners opportunities to create better performing models, more data will have to be processed - causing an increased computational burden in the learning process. Which increase is non-linear: due to the curse of dimensionality the computational burden can get large, quickly. Even though the field has for long relied on computer processing speed steadily increasing, obeying Moore's law, the rate of advancement will inevitably start declining - and in fact already has. Self-evidently, many technological advancements can still be realised, be it either in the silicon world or in a post-silicon world, in which perhaps forms of quantum computing might become predominant. But what is certain, is that besides the technological opportunities the chip-making industry still has, there also exist strict physical limitations as to how fast we can get. So, besides leveraging faster hardware, we are also going to have to make our software smarter. Instead of increasing just our computational power, methods for reducing the computational burden in the first place are desired. Thus the need for pre-processing techniques and data reduction algorithms is instigated.

% Feature Selection
\textbf{Feature Selection} is such a domain that focuses on reducing the overall learning processing burden. By figuring out what dimensions are relevant to the learning task, which ones are redundant and which ones are completely noise with respect to the learning task, a smaller dataset can be obtained by means of pre-processing step. This is contrary to the domain of dimensionality reduction, in which data is projected onto a space of a smaller dimensionality - but losing the exact representation of the data distribution. In Feature Selection, we aim to obtain a binary decision about which dimensions to keep, in the original data space. Aside from reducing dataset size by cutting off dimensions, in some cases the generalization ability of a learning model can even be improved: the learning model can better learn the data distribution by use less but more meaningful dimensions with less noise. This makes the benefit of learning on a subset of the available features two-fold: model fitting and prediction can be both faster and more accurate. To select relevant dataset features, a wide variety of strategies exist. One is to assign a scoring to each dimension and keep only the most relevant ones - such a ranking is called a \textbf{feature ranking}.

% Existing literature


% Evaluation
% But whilst more data presents many opportunities for creating better performing models, learning from a larger amount of samples also means coping with a larger computational burden. Practitioners of automated learning in both the industrial and scientific domains find themselves requiring sophisticated solutions to process the data at hand. 

% why evaluate feature rankers

% what we do in this paper: chapter setup.

\section{Related work}
% other papers that; talked about evaluating feature selection

% papers that take each metric separately; stability, etc.

% research scope

% THEORY
\section{Motivations for feature ranking}
% motivation for ranking and weighting features:
    % (1) feature selection
    % (2) interpretable AI
    \subsection{Feature selection}
    % so many dimensions...
    % not to be confused with: dimensionality reduction (PCA / t-SNE)
    \subsection{Interpretable AI}

\section{Methods for feature ranking}
Feature Rankings can be constructed by running a separate statistical operation on the dataset, before running any learning algorithms, or as part of a learning algorithm itself. In some cases, running a learning algorithm to 

    \subsection{Embedded}
    % early methods:
        % - ridge regression
        % - LASSO
    % (regularization methods)
    \subsection{Wrapper}
    \subsection{Filter / statistical}

% EXPERIMENTS
\section{Experiments}
\subsection{Experiment setup}
\subsection{Experiment results}
\section{Discussion}
\section{Conclusion}
\citep{chen_kernel_2018}

\bibliographystyle{abbrv}
\bibliography{references}

\begin{appendices}

\end{appendices}

\end{document}
