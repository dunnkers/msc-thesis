\providecommand{\main}{./report}
\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Introduction}\label{section:introduction}

% more data: many domains
In this day and age, more data is available than ever before in many domains \citep{sagiroglu_big_2013}. In the biomedical domain sensory devices such as MRI or PET scanners are getting ever more accurate - requiring more storage space to store higher resolution data. In the financial domain, markets are operating at increasingly low time intervals - requiring storage and analysis of data at a higher time resolution than before. The internet too, sees increasing amounts of traffic world-wide and is producing immense amounts of data every day. Applications of Machine Learning are common in all these domains: training predictive models by learning from example data is able to give us interesting insights that can improve both economy and the quality of human lives. By the nature of models that learn from examples, performance is often better when a larger amount of examples is available. But it shall get clear that larger quantities of data presents itself not as solely beneficial; but rather- a mixed blessing.



% ML can benefit: but also harder.
The field of Machine Learning has seen vast increases in dataset sizes: both in terms of sample size and amount of dimensions. Although the availability of more data presents practitioners with opportunities to create better performing models, more data will have to be processed - causing an increased computational burden in the learning process. This increase is nonlinear: due to the curse of dimensionality, the computational burden can get large, quickly. Even though the field has for long relied on computer processing speed steadily increasing, obeying Moore's law, the rate of advancement will inevitably start declining - and in fact already has \citep{theis_end_2017}. Self-evidently, many technological advancements can still be realised, either in the silicon world or in a post-silicon world, in which perhaps forms of quantum computing might become predominant \citep{britt_high-performance_2017}. But what is certain, is that besides the technological opportunities the chip-making industry still has, there also exist strict physical limitations as to how fast computer processing can get. So, besides leveraging faster hardware, we are also going to have to make our software smarter. Instead of increasing just our computational power, methods for reducing the computational burden in the first place are desired. Thus, the need for preprocessing techniques and data reduction algorithms is instigated.



% Feature Selection
\textbf{Feature selection} is such a domain that focuses on reducing the overall learning processing burden \citep{guyon_introduction_2003}. By figuring out what dimensions are relevant to the learning task, which ones are redundant, and which ones are completely noisy with respect to the learning task, a smaller dataset can be obtained by means of a preprocessing step. This is contrary to the domain of dimensionality reduction, in which data is projected onto a space of smaller dimensionality - but losing the exact representation of the data distribution. In Feature Selection, we aim to obtain a binary decision about which dimensions to keep, in the original data space. Aside from reducing dataset size by cutting off dimensions, in some cases the generalization ability of a learning model can even be improved: the learning model can better learn the data distribution by using less but more meaningful dimensions with less noise. This makes the benefit of learning on a subset of the available features two-fold: model fitting and prediction can be both faster and more accurate. To select relevant dataset features, a wide variety of strategies exist. One is to assign a scoring to each dimension and keep only the most relevant ones - such a ranking is called a feature ranking.



% Feature Ranking
\textbf{Feature ranking} is a broader domain in contrast to feature selection, in which the sole purpose is not to only reduce the dataset size, but to construct a hierarchical order on the importance of features given a specific learning task \citep{duch_comparison_2004}. Many techniques can be employed to create such feature rankings, of which some are substantially faster than others, but might yield sub-optimal results: the choice of a suitable feature ranking algorithm is not trivial in most scenarios. Once such a feature ranking has been constructed, it can be used for various applications. First, a feature selection can be made by removing features that rank below a certain feature importance score threshold; one such naive method would be to cut off any feature that ranks below the average feature importance score. Secondly, the feature ranking can also be used for better Machine Learning model interpretation; in which the feature importance scores help humans better understand the predictions and reasoning of the models - by knowing which features the model found important, it can better be understood how the model made the decision that it has. This second application is part of the bigger domain of Interpretable Artificial Intelligence, or more commonly \textit{Interpretable \gls{ai}}, which has in recent times become ever more relevant \citep{ghosh_interpretable_2020}. Interpretable \gls{ai} aims to explain models that were before considered `black box' models, making them more transparent to the user.



% Problem statement
\textbf{Evaluation} on the performance of feature ranking algorithms has been conducted in many different ways. Most authors used a `validation' estimator, which was trained on a chosen subset of the dataset to then see how the estimator performed given this feature subset. A ranker is desired, then, that ranks the most predictive features highest, preferably in a reasonable amount of time. In this way, we get a feature subset that is as small as possible, that gives us the highest possible predictive power. This evaluation technique, however, might not be systematic enough. Across papers, many different validation estimators are used, which make the results across papers subsequently incomparable to one another. Researchers might also benefit from more extensive and systematic evaluation by use of synthetic datasets. By manually controlling many aspects of the dataset, such as noise levels, the complexity of the data distribution to be learned and the amount of informative features, a comprehensive evaluation on the feature ranking algorithm behavior and characteristics can be made. In this way, by employing synthetic datasets, the exact informative features to be ranked as relevant can be known \textit{\gls{apriori}}, i.e. before conducting the feature ranking operation. Such new and possibly useful metrics can be employed to evaluate feature rankings independent of any validation estimator.



% In this paper
\textbf{In this paper}, a comprehensive comparative experiment on feature rankers is conducted using both real-world and synthetic datasets, employing new evaluation metrics on the synthetic datasets by knowing the relevant feature apriori. Both classical and more recent feature ranking algorithms are included, using methods that originally reside in both the statistical and feature selection domains. By systematically generating synthetic datasets that are specifically designed to vary in various relevant data properties, various characteristics of the feature ranking algorithms can be assessed and estimated. To employ such a large-scale benchmark, a software framework was built to facilitate such testing. The framework was released as open-source, freely available software.


% Research question
The \textbf{research question} which is to be answered is as follows. ``How do we evaluate Feature Ranking algorithms in a way that is systematic, comprehensive, and emphasizes the differences between the algorithms in a meaningful way?'' Currently, Feature Ranking and Feature Selection algorithms are evaluated in many ways. This research aims to find a methodology that is both meaningful and applicable to many algorithms.
% The goal...
The goal is to arrive at a scientifically sound benchmarking method for feature ranking methods, taking advantage of \textit{\gls{apriori}} information wherever possible. Up to our knowledge, no current literature exists that is aimed at the evaluation of Feature- Ranking and Selection algorithms, that takes such a comprehensive approach and proposes novel evaluation metrics. An important sub-goal in constructing such a benchmarking method is the development of a pipeline implementing such a new benchmarking method and analysing its results.


% Research scope
\textbf{The scope} of this research can be defined as follows. Feature Ranking methods are evaluated that work on tabular datasets and require example data including prediction targets, i.e. only \textit{supervised} algorithms are considered. Furthermore, all considered datasets are \textit{tabular}, that is, no underlying data structures such as linked- or streaming data is assumed. The considered dataset prediction tasks are \textit{regression} and \textit{classification} - which limits the considered ranking methods to these tasks as well. Eight out of fourteen classification datasets perfectly balanced classes, the other six have varying levels of imbalance (see Appendix). All considered Feature Ranking methods are \textit{global} rankers. Meaning that all methods construct rankings for the full dataset, i.e., no instance-based methods are included. Lastly, all three ranking types are supported and are considered in the research, i.e. the research considers feature importance-, feature support- and feature ranking vectors.



% Paper contribution
\textbf{The contribution} of this paper is multiple-fold. Firstly, the inclusion of many feature ranking algorithms and many datasets makes it possible to make more meaningful comparisons between feature ranking algorithms, which would not have been possible across existing papers due to the lack of a single evaluation standard. Secondly, the proposal of a new evaluation standard also makes it possible for other authors to conduct experiments in reproducible manner. This subsequently enables readers to compare results across papers - saving time but also allowing more thorough comparative analysis on which feature ranker is best suited for a given dataset. Third, the new evaluation standard was implemented and packaged in an easy-to-use open-source software package, distributed as a Python  pip package manager distribution on the PyPi platform\footnote{\href{https://pypi.org/project/fseval/}{https://pypi.org/project/fseval/}}.



% Chapter setup
Chapters in this paper are structured as follows. First, motivations for both Feature Ranking and the evaluation thereof are given in Chapter~\ref{section:motivations}. Second, an analysis on previous work in the literature is conducted, in Chapter~\ref{section:related-work}. Third, various methods for creating feature rankings are presented, such that a grasp can be obtained on the overall mechanics of the methods. This is done is Chapter~\ref{section:methods}. Fourth, an insight into how feature rankings are evaluated is gained; and a new method is presented afterwards, in Chapter~\ref{section:evaluation}. Next, comments are made on the construction of a benchmarking pipeline in Chapter~\ref{section:pipeline}. Then, this new standard is applied in an experiment, which setup and results are explained in Chapter~\ref{section:experiments}. Lastly, the paper is concluded by a discussion in Chapter~\ref{section:discussion} and a conclusion in Chapter~\ref{section:conclusion}.


\biblio
\end{document}
