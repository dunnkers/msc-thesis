\providecommand{\main}{./report}
\documentclass[../main.tex]{subfiles}
\begin{document}


\section{Discussion}\label{section:discussion}
In the following section, a brief discussion is held on the project result. Instead of interpreting the results directly, a broad view will be taken on the achieved results. Because the contribution of the paper consists of multiple elements, each is discussed separately.

%%% Evaluation methodology
The newly proposed \textbf{evaluation methodology} consists out of two main aspects. On the one hand, new evaluation metrics were created and proposed, making use of \gls{apriori} knowledge on relevant features. On the other hand, recommendations were made for the inclusion of existing metrics in the evaluation process of feature- rankers and selectors. Like it was discussed in the related work- and motivation sections, there is an interest in creating better evaluation metrics. There indeed exists a gap in the literature which is to be filled: a discussion on the usefulness of new evaluation metrics w.r.t. the feature- ranking and selection benchmarking process. Seemingly, the new metrics are of some use, and can be used to foretell the validation performance. It must be said, however, that the new evaluation metrics must probably always be accompanied by real-world data analysis with the use of validation estimators, in order to confirm the found results using the \gls{apriori} knowledge.

% Synclf - uniform feature importances problem
An important caveat in the newly proposed evaluation metrics is the assumption of \textit{uniform} feature importance. In the synthetically generated \textit{classification} datasets, grouped as the `Synclf' datasets, the informative features are known \gls{apriori}, thanks to the \texttt{make\_classification} function in sklearn. It is assumed, however, that all informative features do weight equally, i.e., have a uniform distribution w.r.t. feature importance. All rankers are scored accordingly to this uniformly distributed feature importance scores. This assumption might unjustly rank feature rankers as being less performant. This happens, for example, when a ranker assigned many small scores to irrelevant features and thus also a smaller score to the relevant features - but still getting the feature ranking (order) correct. When combining the \gls{apriori} scores with the validation scores, however, a clearer picture be obtained. Nonetheless, for the reason stated above, caution should be taken into interpreting the R\textsuperscript{2}- and log loss scores for the synthetically generated classification datasets. What is certainly still useful to practitioners, however, are the charts obtained by plotting the estimated feature importances against the ground-truth feature importances.

% Synreg
In the case of the synthetically generated \textit{regression} datasets, this caveat does not hold. This is because in the case of synthetically generated regression datasets, the actual ground-truth desired feature importance coefficients are known. This is due to the \texttt{make\_regression} function. This is a big plus for the `Synreg' datasets, giving them an extra amount of trustworthiness.


%%% Pipeline
Next, the created \textbf{pipeline} is discussed. Arguably, this is the biggest contribution of the project. The initial goals before building the pipeline were several. The pipeline had to be flexible, to support many different feature rankers, datasets, and validation estimators. It was also desired to have the data- collection and visualization steps largely automatized, such that new experiments could be added easily. And all this was desired to be scalable, allowing the pipeline to be run on many processors and on many machines simultaneously. In the pipeline implementation that was built, all above features were addressed. When the pipeline is run, a practitioner has to complete only little steps to visualize and interpret the data, due to integration with the online dashboard. The pipeline is open-source and released as a PyPi package, such that a practitioner can also easily extend the pipeline to their own needs.

% both classification and regression support
A noteworthy remark about the pipeline is to be made, however. The pipeline might have tried to tackle too many problems in a single codebase. Although the pipeline is, in the end very flexible, lots of code exists to support both regression- and classification datasets. This does make the pipeline more useful for a wider audience, but also forces the programmer to give in to simplicity. Evaluation metrics have to be implemented twice and validation estimators have to account for both learning tasks. All result data also has to be kept apart such not to intermingle the two. An upside of having included both learning tasks, however, is that it is now relatively easy to add support for another learning task. Since the codebase already deals with \textit{multiple} learning tasks, the difference between choosing between two and more is not big. Learning tasks such as clustering could be supported in this way. 


%%% Experiment
Lastly, the \textbf{experiment} is discussed. The experiment is to be an implementation of the proposed methodology, and an application of the built pipeline. Although at first outset, the goal in this paper was to benchmark as many as possible feature rankers, at a specific point in time the scope was reduced to contain a more finite set of feature rankers. This is mainly due to the fact that even though the literature on feature- rankers and selectors is very extensive, the implementations are scarce. Only in rare cases, an up-to-date software package for the designated feature- ranker or selector exists. In most cases, software packages are out dated and work with older versions of sklearn. In other cases, only an R implementation can be found.

Nonetheless, the experiment does allow a practitioner to draw useful conclusions. Still, the experiment is more extensive than what is seen in many algorithm proposals in the literature. Many datasets are included, both of the real-world and synthetic type. But most importantly, using the newly proposed evaluation methodology and pipeline implementation that is freely available, new experiments can easily be added. Even, direct comparison is possible when data is uploaded to the same dashboard - allowing this experiment to be a starting place for further experimentation. In other words, with a proof-of-concept experiment in place, the field is open for more comprehensive experiments.

\biblio
\end{document}
