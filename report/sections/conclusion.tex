\providecommand{\main}{./report}
\documentclass[../main.tex]{subfiles}
\begin{document}


\section{Conclusion}\label{section:conclusion}
First, a concluding note will be given. Lastly, possible points of improvement for future work will be elaborated upon.

\subsection{Concluding note}
% Literature gap
In this paper, it was investigated how best to evaluate feature- rankers and selectors. A selection of literature was considered, and their evaluation methods were compared. Traditionally, papers paid little attention to using \gls{apriori} knowledge nor performing stability analysis. Given this lack, one can speak of a gap in the literature - there is an interesting opportunity for investigating new methods.
% New evaluation methodology
After this opportunity was identified, new evaluation methods were proposed. In this way, a new methodology for evaluating feature- rankers and selectors was compiled in this paper. The main opportunities lie in the use of \gls{apriori} knowledge, performing stability analysis, and summarizing the validation performance in a novel way using a scalar metric.

% Pipeline & Benchmark
An extensive benchmarking pipeline was constructed, implementing this new evaluation methodology. The pipeline is scalable, modular, and easy to configure. The pipeline is available as open-source free software, called `fseval'. A comprehensive experiment was also run using the pipeline, illustrating the pipeline capability's in a concrete benchmark. Using an online dashboard, the benchmark can easily be extended in the future. The results can also be interactively explored. Strong overall performers for classification datasets are Decision Tree, XGBoost, FeatBoost, and Infinite Selection. 

% Feature Ranking conclusion
Feature ranking is an ever more relevant problem in a world where data and machine learning pose a prevalent role, useful for both feature selection and interpretable AI. With many feature ranking algorithms available, a comprehensive analysis is required to pick a suitable method given the context. But, when all relevant facets are highlighted and analyzed in a comprehensive feature ranking evaluation pipeline, authors and users can be more deliberate in arguing for any algorithmâ€™s superiority.

\subsection{Limitations and Future work}
In the future, authors might build upon the work done in this paper. The following outlines the limitations of this paper and ideas for extending the work done in this project.

\textbf{Limitations} of the paper were the following.
\begin{itemize}
    \item The proposed methodology, including new evaluation metrics, could have enjoyed more mathematical argumentative support. Currently, the evaluation metrics \textit{are} outlined mathematically, but no theoretical predictions about their behavior are made. Instead, their usefulness is proven only in the experiment section, through empirical observation. Quite possibly, the usefulness of certain evaluation metrics could have been foretold mathematically.
    \item Another important caveat is the fact that the considered feature rankers were not hyper-parameter optimized for the experiment datasets. All feature rankers were run at their default settings. Fair to say, this completely invalidates some rankers. Since some rankers do not function properly without hyper-parameter optimization, their performance can be misleadingly bad. In this experiment, this is the case with Boruta, for example. One could also argue, however, that the algorithms should be able to function well at their defaults. A practitioner does not always have the luxury to perform a computationally expensive hyper-parameter optimization process.
    \item The feature support and feature rankings were less extensively evaluated than feature importance estimations. For example, the feature support estimations could have also had a more sophisticated metric devised for its evaluation. Currently, only the mean validation scores of the feature subsets are taken into account, but ideally, also the \textit{amount of selected features} would have been taken into account in the metric.
    \item Lastly, the experiment could have been more extensive. Currently, only three datasets are multioutput, meaning the rankers supporting multioutput get little comparison material. All these datasets were also of regression type. Given enough processing power, ideally the entire OpenML-CC18 benchmark suite would be run. Moreover, more synthetic datasets would be tested using a range of parameter settings.
\end{itemize}

\textbf{Ideas} for future work are several.
\begin{itemize}
    \item The pipeline can be extended in several straight-forward ways. (1) Firstly, more dataset adapters could be added. An interesting platform to support is Kaggle, although adapters for loading local- or remote CSV or JSON would also be useful. (2) Secondly, the pipeline could integrate with more back-ends aside from WandB. OpenML can also be used, for example, to upload metrics and experimental results to. Alternatively, data could be uploaded directly to a database of some kind, like MySQL. In this way, by adding more integrations to the existing benchmarking pipeline, the framework could become a truly versatile benchmarking tool for any feature ranking algorithm. This also the last pipeline improvement idea. (3) Lastly, one might even extract the general pipeline and benchmarking capabilities of the framework and use them for general-purpose \gls{ml} benchmarking. This would turn the framework from being just a feature ranking evaluation framework into a generic framework for testing \gls{ml} algorithms. 
    \item The evaluation process with regards to the feature importances ground-truth can enjoy more research. One idea is to normalize the feature importances vectors $\boldsymbol{w}$ and $\boldsymbol{\hat{w}}$ by a \textit{softmax} operation, instead of normalizing only by the sum of the vector. Using the current normalization method, negative values are not allowed in the feature importance vector. Although this is rare, it might occur. Furthermore, authors might also want to try to \textit{weight} the apriori scorings such as the R\textsuperscript{2} and log loss scores. A weighting could make sure, then, that the relevant features are assigned more weight in the scoring process, i.e., algorithms are rewarded more for getting those features right rather than the irrelevant features.
    \item Another idea is to apply the new evaluation methodology to interpretable AI algorithms in a more extensive way. Currently, only one method related to interpretable AI was included, TabNet. No special considerations were made for such rankers, but many interpretable AI methods support ranking features on a per-instance basis. Instance-based feature importance scoring is not considered in this paper's context. Because, however, the goal of ranking feature importances is similar in the domain of interpretable AI, the benchmarking framework can be made compatible relatively easily with instance-based methods.
\end{itemize}

\biblio
\end{document}
