
@article{guyon_design_2003,
	title = {Design of experiments for the {NIPS} 2003 variable selection benchmark},
	url = {https://www.semanticscholar.org/paper/Design-of-experiments-for-the-NIPS-2003-variable-Guyon/b979fa88ca448fb08633f961131f45214b1cf109},
	language = {en},
	journal = {The Journal of Machine Learning Research},
	author = {Guyon, Isabelle},
	year = {2003},
	keywords = {benchmark, read 40\%, synthetic datasets},
	pages = {30},
}

@book{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	publisher = {Self-Published},
	author = {Biewald, Lukas},
	year = {2020},
}

@inproceedings{chen_xgboost_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {https://doi.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2021-06-16},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {785--794},
}

@article{meinshausen_stability_2009,
	title = {Stability {Selection}},
	url = {http://arxiv.org/abs/0809.2932},
	abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis is notoriously difficult, especially for high-dimensional data. We introduce stability selection. It is based on subsampling in combination with (high-dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularisation for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for randomised Lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original Lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
	urldate = {2021-06-16},
	journal = {arXiv:0809.2932 [stat]},
	author = {Meinshausen, Nicolai and Buehlmann, Peter},
	month = may,
	year = {2009},
	note = {arXiv: 0809.2932},
}

@article{satyanarayan_vega-lite_2017,
	title = {Vega-{Lite}: {A} {Grammar} of {Interactive} {Graphics}},
	volume = {23},
	issn = {1941-0506},
	shorttitle = {Vega-{Lite}},
	doi = {10.1109/TVCG.2016.2599030},
	abstract = {We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Brushes, Data visualization, Encoding, Grammar, Information visualization, Transforms, Visualization, declarative specification, interaction, systems, toolkits},
	pages = {341--350},
}

@inproceedings{ristov_superlinear_2016,
	title = {Superlinear speedup in {HPC} systems: {Why} and when?},
	shorttitle = {Superlinear speedup in {HPC} systems},
	abstract = {The speedup is usually limited by two main laws in high-performance computing, that is, the Amdahl's and Gustafson's laws. However, the speedup sometimes can reach far beyond the limited linear speedup, known as superlinear speedup, which means that the speedup is greater than the number of processors that are used. Although the superlinear speedup is not a new concept and many authors have already reported its existence, most of them reported it as a side effect, without explaining why and how it is happening. In this paper, we analyze several different superlinear speedup types and define a taxonomy for them. Additionally, we present several explanations and cases of superlinearity existence for different types of granular algorithms (tasks), which means that they can be divided into many sub-tasks and scattered to the processors for execution. Apart from frequent explanation that having more cache memory in parallel execution is the main reason, we summarize other different effects that cause the superlinearity, including the superlinear speedup in cloud virtual environment for both vertical and horizontal scaling.},
	booktitle = {2016 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Ristov, Sasko and Prodan, Radu and Gusev, Marjan and Skala, Karolj},
	month = sep,
	year = {2016},
	keywords = {Cache memory, Clocks, Cloud computing, Complexity theory, Program processors, Synchronization, Throughput, load, parallel and distributed processing, performance},
	pages = {889--898},
}

@article{bischl_openml_2019,
	title = {{OpenML} {Benchmarking} {Suites}},
	url = {http://arxiv.org/abs/1708.03731},
	abstract = {Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. Therefore, we advocate the use of curated, comprehensive suites of machine learning tasks to standardize the setup, execution, and reporting of benchmarks. We enable this through software tools that help to create and leverage these benchmarking suites. These are seamlessly integrated into the OpenML platform, and accessible through interfaces in Python, Java, and R. OpenML benchmarking suites are (a) easy to use through standardized data formats, APIs, and client libraries; (b) machine-readable, with extensive meta-information on the included datasets; and (c) allow benchmarks to be shared and reused in future studies. We also present a first, carefully curated and practical benchmarking suite for classification: the OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18).},
	urldate = {2021-06-14},
	journal = {arXiv:1708.03731 [cs, stat]},
	author = {Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin},
	month = sep,
	year = {2019},
	note = {arXiv: 1708.03731
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vanschoren_openml_2014,
	title = {{OpenML}: networked science in machine learning},
	volume = {15},
	issn = {1931-0145},
	shorttitle = {{OpenML}},
	url = {https://doi.org/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
	number = {2},
	urldate = {2021-06-14},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	month = jun,
	year = {2014},
	pages = {49--60},
}

@book{yadan_hydra_2019,
	title = {Hydra - {A} framework for elegantly configuring complex applications},
	url = {https://github.com/facebookresearch/hydra},
	publisher = {Self-Published},
	author = {Yadan, Omry},
	year = {2019},
}

@article{nogueira_stability_2018,
	title = {On the {Stability} of {Feature} {Selection} {Algorithms}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/17-514.html},
	number = {174},
	urldate = {2021-06-12},
	journal = {Journal of Machine Learning Research},
	author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
	year = {2018},
	keywords = {read 20\%, read 40\%, read 60\%},
	pages = {1--54},
}

@article{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2021-04-22},
	journal = {arXiv:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, to read},
}

@inproceedings{kira_feature_1992,
	title = {The {Feature} {Selection} {Problem}: {Traditional} {Methods} and a {New} {Algorithm}},
	shorttitle = {The {Feature} {Selection} {Problem}},
	abstract = {For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Rellef which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as nonoptimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems.},
	booktitle = {{AAAI}},
	author = {Kira, K. and Rendell, L.},
	year = {1992},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
}

@inproceedings{almuallim_learning_1991,
	title = {Learning {With} {Many} {Irrelevant} {Features}},
	abstract = {In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires {\textbackslash}Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  +  p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...},
	booktitle = {In {Proceedings} of the {Ninth} {National} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Almuallim, Hussein and Dietterich, Thomas G.},
	year = {1991},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {547--552},
}

@inproceedings{bradley_feature_1998,
	title = {Feature {Selection} via {Concave} {Minimization} and {Support} {Vector} {Machines}},
	abstract = {Computational comparison is made between two feature selection approaches for finding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassified points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassified points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is us...},
	booktitle = {Machine {Learning} {Proceedings} of the {Fifteenth} {International} {Conference}({ICML} ’98},
	publisher = {Morgan Kaufmann},
	author = {Bradley, P. S. and Mangasarian, O. L.},
	year = {1998},
	keywords = {novel algorithm, read 20\%, to read},
	pages = {82--90},
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	issn = {1532-4435},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	number = {null},
	journal = {The Journal of Machine Learning Research},
	author = {Guyon, Isabelle and Elisseeff, André},
	month = mar,
	year = {2003},
	pages = {1157--1182},
}

@article{urbanowicz_relief-based_2018,
	title = {Relief-based feature selection: {Introduction} and review},
	volume = {85},
	issn = {1532-0464},
	shorttitle = {Relief-based feature selection},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046418301400},
	doi = {10.1016/j.jbi.2018.07.014},
	abstract = {Feature selection plays a critical role in biomedical data mining, driven by increasing feature dimensionality in target problems and growing interest in advanced but computationally expensive methodologies able to model complex associations. Specifically, there is a need for feature selection methods that are computationally efficient, yet sensitive to complex patterns of association, e.g. interactions, so that informative features are not mistakenly eliminated prior to downstream modeling. This paper focuses on Relief-based algorithms (RBAs), a unique family of filter-style feature selection algorithms that have gained appeal by striking an effective balance between these objectives while flexibly adapting to various data characteristics, e.g. classification vs. regression. First, this work broadly examines types of feature selection and defines RBAs within that context. Next, we introduce the original Relief algorithm and associated concepts, emphasizing the intuition behind how it works, how feature weights generated by the algorithm can be interpreted, and why it is sensitive to feature interactions without evaluating combinations of features. Lastly, we include an expansive review of RBA methodological research beyond Relief and its popular descendant, ReliefF. In particular, we characterize branches of RBA research, and provide comparative summaries of RBA algorithms including contributions, strategies, functionality, time complexity, adaptation to key data characteristics, and software availability.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Journal of Biomedical Informatics},
	author = {Urbanowicz, Ryan J. and Meeker, Melissa and La Cava, William and Olson, Randal S. and Moore, Jason H.},
	month = sep,
	year = {2018},
	keywords = {benchmark, read 20\%},
	pages = {189--203},
}

@article{urbanowicz_benchmarking_2018,
	title = {Benchmarking relief-based feature selection methods for bioinformatics data mining},
	volume = {85},
	issn = {1532-0464},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046418301412},
	doi = {10.1016/j.jbi.2018.07.015},
	abstract = {Modern biomedical data mining requires feature selection methods that can (1) be applied to large scale feature spaces (e.g. ‘omics’ data), (2) function in noisy problems, (3) detect complex patterns of association (e.g. gene-gene interactions), (4) be flexibly adapted to various problem domains and data types (e.g. genetic variants, gene expression, and clinical data) and (5) are computationally tractable. To that end, this work examines a set of filter-style feature selection algorithms inspired by the ‘Relief’ algorithm, i.e. Relief-Based algorithms (RBAs). We implement and expand these RBAs in an open source framework called ReBATE (Relief-Based Algorithm Training Environment). We apply a comprehensive genetic simulation study comparing existing RBAs, a proposed RBA called MultiSURF, and other established feature selection methods, over a variety of problems. The results of this study (1) support the assertion that RBAs are particularly flexible, efficient, and powerful feature selection methods that differentiate relevant features having univariate, multivariate, epistatic, or heterogeneous associations, (2) confirm the efficacy of expansions for classification vs. regression, discrete vs. continuous features, missing data, multiple classes, or class imbalance, (3) identify previously unknown limitations of specific RBAs, and (4) suggest that while MultiSURF∗ performs best for explicitly identifying pure 2-way interactions, MultiSURF yields the most reliable feature selection performance across a wide range of problem types.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Journal of Biomedical Informatics},
	author = {Urbanowicz, Ryan J. and Olson, Randal S. and Schmitt, Peter and Meeker, Melissa and Moore, Jason H.},
	month = sep,
	year = {2018},
	keywords = {benchmark, novel algorithm, read 40\%, read 60\%},
	pages = {168--188},
}

@article{arik_tabnet_2020,
	title = {{TabNet}: {Attentive} {Interpretable} {Tabular} {Learning}},
	shorttitle = {{TabNet}},
	url = {http://arxiv.org/abs/1908.07442},
	abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
	urldate = {2020-09-29},
	journal = {arXiv:1908.07442 [cs, stat]},
	author = {Arik, Sercan O. and Pfister, Tomas},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.07442},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, novel algorithm, read 20\%, read 40\%},
}

@article{aha_instance-based_1991,
	title = {Instance-based learning algorithms},
	volume = {6},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00153759},
	doi = {10.1007/BF00153759},
	abstract = {Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-world databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.},
	language = {en},
	number = {1},
	urldate = {2021-06-09},
	journal = {Machine Learning},
	author = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
	month = jan,
	year = {1991},
	pages = {37--66},
}

@inproceedings{yu_feature_2003,
	address = {Washington, DC, USA},
	series = {{ICML}'03},
	title = {Feature selection for high-dimensional data: a fast correlation-based filter solution},
	isbn = {978-1-57735-189-4},
	shorttitle = {Feature selection for high-dimensional data},
	abstract = {Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the {Twentieth} {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Yu, Lei and Liu, Huan},
	month = aug,
	year = {2003},
	pages = {856--863},
}

@article{maldonado_weber_2009,
	title = {Weber, {R}.: {A} wrapper method for feature selection using support vector machines. {Inf}. {Sci}. 179(13), 2208-2217},
	volume = {179},
	shorttitle = {Weber, {R}.},
	doi = {10.1016/j.ins.2009.02.014},
	abstract = {We introduce a novel wrapper Algorithm for Feature Selection, using Support Vector Machines with kernel functions. Our method is based on a sequential backward selection, using the number of errors in a validation subset as the measure to decide which feature to remove in each iteration. We compare our approach with other algorithms like a filter method or Recursive Feature Elimination SVM to demonstrate its effectiveness and efficiency.},
	journal = {Inf. Sci.},
	author = {Maldonado, Sebastián and Weber, Richard},
	month = jun,
	year = {2009},
	pages = {2208--2217},
}

@article{ambroise_selection_2002,
	title = {Selection bias in gene extraction on the basis of microarray gene-expression data},
	volume = {99},
	copyright = {Copyright © 2002, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/99/10/6562},
	doi = {10.1073/pnas.102102699},
	abstract = {In the context of cancer diagnosis and treatment, we consider the problem of constructing an accurate prediction rule on the basis of a relatively small number of tumor tissue samples of known type containing the expression data on very many (possibly thousands) genes. Recently, results have been presented in the literature suggesting that it is possible to construct a prediction rule from only a few genes such that it has a negligible prediction error rate. However, in these results the test error or the leave-one-out cross-validated error is calculated without allowance for the selection bias. There is no allowance because the rule is either tested on tissue samples that were used in the first instance to select the genes being used in the rule or because the cross-validation of the rule is not external to the selection process; that is, gene selection is not performed in training the rule at each stage of the cross-validation process. We describe how in practice the selection bias can be assessed and corrected for by either performing a cross-validation or applying the bootstrap external to the selection process. We recommend using 10-fold rather than leave-one-out cross-validation, and concerning the bootstrap, we suggest using the so-called .632+ bootstrap error estimate designed to handle overfitted prediction rules. Using two published data sets, we demonstrate that when correction is made for the selection bias, the cross-validated error is no longer zero for a subset of only a few genes.},
	language = {en},
	number = {10},
	urldate = {2021-06-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ambroise, Christophe and McLachlan, Geoffrey J.},
	month = may,
	year = {2002},
	pmid = {11983868},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	pages = {6562--6566},
}

@book{davis_undecidable_2004,
	title = {The {Undecidable}: {Basic} {Papers} on {Undecidable} {Propositions}, {Unsolvable} {Problems} and {Computable} {Functions}},
	isbn = {978-0-486-43228-1},
	shorttitle = {The {Undecidable}},
	abstract = {An anthology of fundamental papers on undecidability and unsolvability, this classic reference opens with Gödel\&\#39;s landmark 1931 paper demonstrating that systems of logic cannot admit proofs of all true assertions of arithmetic. Subsequent papers by Gödel, Church, Turing, and Post single out the class of recursive functions as computable by finite algorithms. 1965 edition.},
	language = {en},
	publisher = {Courier Corporation},
	author = {Davis, Martin},
	month = jan,
	year = {2004},
	note = {Google-Books-ID: qW8x7sQ4JXgC},
}

@article{kursa_feature_2010,
	title = {Feature {Selection} with the {Boruta} {Package}},
	volume = {36},
	copyright = {Copyright (c) 2009 Miron B. Kursa, Witold R. Rudnicki},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v036i11},
	doi = {10.18637/jss.v036.i11},
	language = {en},
	number = {1},
	urldate = {2021-06-05},
	journal = {Journal of Statistical Software},
	author = {Kursa, Miron B. and Rudnicki, Witold R.},
	month = sep,
	year = {2010},
	note = {Number: 1},
	pages = {1--13},
}

@book{hastie_elements_2009,
	address = {New York},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	url = {https://www.springer.com/gp/book/9780387848570},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	language = {en},
	urldate = {2021-06-03},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
}

@incollection{hansen_interpretability_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretability in {Intelligent} {Systems} – {A} {New} {Concept}?},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_3},
	abstract = {The very active community for interpretable machine learning can learn from the rich 50+ year history of explainable AI. We here give two specific examples from this legacy that could enrich current interpretability work: First, Explanation desiderata were we point to the rich set of ideas developed in the ‘explainable expert systems’ field and, second, tools for quantification of uncertainty of high-dimensional feature importance maps which have been developed in the field of computational neuroimaging.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hansen, Lars Kai and Rieger, Laura},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_3},
	keywords = {Interpretable AI, Machine learning, Uncertainty quantification},
	pages = {41--49},
}

@article{koppen_curse_2009,
	title = {The {Curse} of {Dimensionality}},
	doi = {10.1007/978-0-387-39940-9_133},
	abstract = {In this text, some question related to higher dimensional geometrical spaces will be discussed. The goal is to give the reader a feeling for geometric distortions related to the use of such spaces (e.g. as search spaces).},
	journal = {Encyclopedia of Measurement and Statistics},
	author = {Köppen, Mario},
	month = apr,
	year = {2009},
}

@book{horst_palmerpenguins_2020,
	title = {palmerpenguins: {Palmer} {Archipelago} ({Antarctica}) penguin data},
	url = {https://allisonhorst.github.io/palmerpenguins/},
	publisher = {Self-Published},
	author = {Horst, Allison Marie and Hill, Alison Presmanes and Gorman, Kristen B.},
	year = {2020},
	doi = {10.5281/zenodo.3960218},
}

@book{oneil_weapons_2016,
	title = {Weapons of {Math} {Destruction}: {How} {Big} {Data} {Increases} {Inequality} and {Threatens} {Democracy}},
	isbn = {978-0-553-41882-8},
	shorttitle = {Weapons of {Math} {Destruction}},
	abstract = {NEW YORK TIMES BESTSELLER • A former Wall Street quant sounds the alarm on Big Data and the mathematical models that threaten to rip apart our social fabric—with a new afterword “A manual for the twenty-first-century citizen . . . relevant and urgent.”—Financial Times NATIONAL BOOK AWARD LONGLIST • NAMED ONE OF THE BEST BOOKS OF THE YEAR BY The New York Times Book Review • The Boston Globe • Wired • Fortune • Kirkus Reviews • The Guardian • Nature • On Point We live in the age of the algorithm. Increasingly, the decisions that affect our lives—where we go to school, whether we can get a job or a loan, how much we pay for health insurance—are being made not by humans, but by machines. In theory, this should lead to greater fairness: Everyone is judged according to the same rules. But as mathematician and data scientist Cathy O’Neil reveals, the mathematical models being used today are unregulated and uncontestable, even when they’re wrong. Most troubling, they reinforce discrimination—propping up the lucky, punishing the downtrodden, and undermining our democracy in the process. Welcome to the dark side of Big Data.},
	language = {en},
	publisher = {Crown},
	author = {O'Neil, Cathy},
	month = sep,
	year = {2016},
	note = {Google-Books-ID: NgEwCwAAQBAJ},
	keywords = {Business \& Economics / Statistics, Political Science / Public Policy / General, Social Science / Privacy \& Surveillance},
}

@article{rai_explainable_2020,
	title = {Explainable {AI}: from black box to glass box},
	volume = {48},
	issn = {1552-7824},
	shorttitle = {Explainable {AI}},
	url = {https://doi.org/10.1007/s11747-019-00710-5},
	doi = {10.1007/s11747-019-00710-5},
	language = {en},
	number = {1},
	urldate = {2021-06-01},
	journal = {Journal of the Academy of Marketing Science},
	author = {Rai, Arun},
	month = jan,
	year = {2020},
	pages = {137--141},
}

@article{hu_feature_2018,
	title = {Feature {Selection} for {Optimized} {High}-{Dimensional} {Biomedical} {Data} {Using} an {Improved} {Shuffled} {Frog} {Leaping} {Algorithm}},
	url = {/paper/Feature-Selection-for-Optimized-High-Dimensional-an-Hu-Dai/9d495734dead7b1f982183b08e3ef070e2dd18b1},
	abstract = {High dimensional biomedical datasets contain thousands of features which can be used in molecular diagnosis of disease, however, such datasets contain many irrelevant or weak correlation features which influence the predictive accuracy of diagnosis. Without a feature selection algorithm, it is difficult for the existing classification techniques to accurately identify patterns in the features. The purpose of feature selection is to not only identify a feature subset from an original set of features [without reducing the predictive accuracy of classification algorithm] but also reduce the computation overhead in data mining. In this paper, we present our improved shuffled frog leaping algorithm which introduces a chaos memory weight factor, an absolute balance group strategy, and an adaptive transfer factor. Our proposed approach explores the space of possible subsets to obtain the set of features that maximizes the predictive accuracy and minimizes irrelevant features in high-dimensional biomedical data. To evaluate the effectiveness of our proposed method, we have employed the K-nearest neighbor method with a comparative analysis in which we compare our proposed approach with genetic algorithms, particle swarm optimization, and the shuffled frog leaping algorithm. Experimental results show that our improved algorithm achieves improvements in the identification of relevant subsets and in classification accuracy.},
	language = {en},
	urldate = {2021-06-01},
	journal = {undefined},
	author = {Hu, B. and Dai, Yongqiang and Su, Y. and Moore, P. and Zhang, Xiaowei and Mao, Chengsheng and Chen, J. and Xu, L.},
	year = {2018},
}

@inproceedings{davis_relationship_2006,
	title = {The relationship between {Precision}-{Recall} and {ROC} curves},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	author = {Davis, Jesse and Goadrich, Mark},
	year = {2006},
	pages = {233--240},
}

@article{everingham_pascal_2010,
	title = {The pascal visual object classes (voc) challenge},
	volume = {88},
	number = {2},
	journal = {International journal of computer vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
	year = {2010},
	note = {Publisher: Springer},
	pages = {303--338},
}

@inproceedings{kuncheva_stability_2007,
	title = {A stability index for feature selection.},
	booktitle = {Artificial intelligence and applications},
	author = {Kuncheva, Ludmila I},
	year = {2007},
	pages = {421--427},
}

@article{mohana_chelvan_survey_2016,
	title = {A survey on feature selection stability measures},
	volume = {5},
	number = {1},
	journal = {International Journal of Computer and Information Technology},
	author = {Mohana Chelvan, P and Perumal, Karuppasamy},
	year = {2016},
	pages = {98--103},
}

@inproceedings{boyd_area_2013,
	title = {Area under the precision-recall curve: point estimates and confidence intervals},
	booktitle = {Joint {European} conference on machine learning and knowledge discovery in databases},
	publisher = {Springer},
	author = {Boyd, Kendrick and Eng, Kevin H and Page, C David},
	year = {2013},
	pages = {451--466},
}

@article{kalousis_stability_2007,
	title = {Stability of feature selection algorithms: a study on high-dimensional spaces},
	volume = {12},
	number = {1},
	journal = {Knowledge and information systems},
	author = {Kalousis, Alexandros and Prados, Julien and Hilario, Melanie},
	year = {2007},
	note = {Publisher: Springer},
	pages = {95--116},
}

@article{dunne_solutions_2002,
	title = {Solutions to instability problems with sequential wrapper-based approaches to feature selection},
	journal = {Journal of Machine Learning Research},
	author = {Dunne, Kevin and Cunningham, Padraig and Azuaje, Francisco},
	year = {2002},
	pages = {1--22},
}

@article{cai_online_2020,
	title = {Online {Sufficient} {Dimension} {Reduction} {Through} {Sliced} {Inverse} {Regression}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/18-567.html},
	number = {10},
	journal = {Journal of Machine Learning Research},
	author = {Cai, Zhanrui and Li, Runze and Zhu, Liping},
	year = {2020},
	keywords = {read 20\%, read 40\%},
	pages = {1--25},
}

@article{li_distributed_2020,
	title = {Distributed {Feature} {Screening} via {Componentwise} {Debiasing}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-537.html},
	number = {24},
	journal = {Journal of Machine Learning Research},
	author = {Li, Xingxiang and Li, Runze and Xia, Zhiming and Xu, Chen},
	year = {2020},
	pages = {1--32},
}

@article{tang_feature_2014,
	title = {Feature selection for classification: {A} review},
	journal = {Data classification: Algorithms and applications},
	author = {Tang, Jiliang and Alelyani, Salem and Liu, Huan},
	year = {2014},
	note = {Publisher: CRC press},
	pages = {37},
}

@incollection{mafarja_dragonfly_2020,
	title = {Dragonfly algorithm: theory, literature review, and application in feature selection},
	booktitle = {Nature-{Inspired} {Optimizers}},
	publisher = {Springer},
	author = {Mafarja, Majdi and Heidari, Ali Asghar and Faris, Hossam and Mirjalili, Seyedali and Aljarah, Ibrahim},
	year = {2020},
	pages = {47--67},
}

@incollection{khurma_evolopy-fs_2020,
	title = {{EvoloPy}-{FS}: {An} {Open}-{Source} {Nature}-{Inspired} {Optimization} {Framework} in {Python} for {Feature} {Selection}},
	booktitle = {Evolutionary {Machine} {Learning} {Techniques}},
	publisher = {Springer},
	author = {Khurma, Ruba Abu and Aljarah, Ibrahim and Sharieh, Ahmad and Mirjalili, Seyedali},
	year = {2020},
	pages = {131--173},
}

@incollection{al-tashi_review_2020,
	title = {A {Review} of {Grey} {Wolf} {Optimizer}-{Based} {Feature} {Selection} {Methods} for {Classification}},
	booktitle = {Evolutionary {Machine} {Learning} {Techniques}},
	publisher = {Springer},
	author = {Al-Tashi, Qasem and Rais, Helmi Md and Abdulkadir, Said Jadid and Mirjalili, Seyedali and Alhussian, Hitham},
	year = {2020},
	pages = {273--286},
}

@techreport{koller_toward_1996,
	title = {Toward optimal feature selection},
	institution = {Stanford InfoLab},
	author = {Koller, Daphne and Sahami, Mehran},
	year = {1996},
}

@article{li_feature_2017,
	title = {Feature selection: {A} data perspective},
	volume = {50},
	number = {6},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P and Tang, Jiliang and Liu, Huan},
	year = {2017},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--45},
}

@article{chandrashekar_survey_2014,
	title = {A survey on feature selection methods},
	volume = {40},
	number = {1},
	journal = {Computers \& Electrical Engineering},
	author = {Chandrashekar, Girish and Sahin, Ferat},
	year = {2014},
	note = {Publisher: Elsevier},
	pages = {16--28},
}

@article{solorio-fernandez_review_2020,
	title = {A review of unsupervised feature selection methods},
	volume = {53},
	number = {2},
	journal = {Artificial Intelligence Review},
	author = {Solorio-Fernández, Saúl and Carrasco-Ochoa, J Ariel and Martínez-Trinidad, José Fco},
	year = {2020},
	note = {Publisher: Springer},
	pages = {907--948},
}

@inproceedings{krizek_improving_2007,
	title = {Improving stability of feature selection methods},
	booktitle = {International {Conference} on {Computer} {Analysis} of {Images} and {Patterns}},
	publisher = {Springer},
	author = {Křížek, Pavel and Kittler, Josef and Hlaváč, Václav},
	year = {2007},
	pages = {929--936},
}

@inproceedings{saeys_towards_2008,
	title = {Towards robust feature selection techniques},
	booktitle = {Proceedings of {Benelearn}},
	publisher = {Citeseer},
	author = {Saeys, Yvan and Abeel, Thomas and de Peer, YV},
	year = {2008},
	pages = {45--46},
}

@article{cunningham_dimension_2007,
	title = {Dimension {Reduction}, {Technical} report {UCD}-{CSI}-2007-7},
	journal = {University College Dublin},
	author = {Cunningham, P},
	year = {2007},
}

@article{tang_high-dimensional_2020,
	title = {High-{Dimensional} {Interactions} {Detection} with {Sparse} {Principal} {Hessian} {Matrix}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-071.html},
	number = {19},
	journal = {Journal of Machine Learning Research},
	author = {Tang, Cheng Yong and Fang, Ethan X. and Dong, Yuexiao},
	year = {2020},
	pages = {1--25},
}

@book{alelyani_feature_2013,
	title = {On feature selection stability: {A} data perspective},
	publisher = {Citeseer},
	author = {Alelyani, Salem},
	year = {2013},
}

@article{demsar_statistical_2006,
	title = {Statistical comparisons of classifiers over multiple data sets},
	volume = {7},
	number = {Jan},
	journal = {Journal of Machine learning research},
	author = {Demšar, Janez},
	year = {2006},
	pages = {1--30},
}

@article{guyon_gene_2002,
	title = {Gene selection for cancer classification using support vector machines},
	volume = {46},
	number = {1-3},
	journal = {Machine learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	year = {2002},
	note = {Publisher: Springer},
	pages = {389--422},
}

@article{geurts_extremely_2006,
	title = {Extremely randomized trees},
	volume = {63},
	number = {1},
	journal = {Machine learning},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	year = {2006},
	note = {Publisher: Springer},
	pages = {3--42},
}

@article{bolon-canedo_ensembles_2019,
	title = {Ensembles for feature selection: {A} review and future trends},
	volume = {52},
	journal = {Information Fusion},
	author = {Bolón-Canedo, Verónica and Alonso-Betanzos, Amparo},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {1--12},
}

@inproceedings{das_filters_2001,
	title = {Filters, wrappers and a boosting-based hybrid for feature selection},
	volume = {1},
	booktitle = {Icml},
	author = {Das, Sanmay},
	year = {2001},
	pages = {74--81},
}

@article{hsu_hybrid_2011,
	title = {Hybrid feature selection by combining filters and wrappers},
	volume = {38},
	number = {7},
	journal = {Expert Systems with Applications},
	author = {Hsu, Hui-Huang and Hsieh, Cheng-Wei and Lu, Ming-Da},
	year = {2011},
	note = {Publisher: Elsevier},
	pages = {8144--8150},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} learning in {Python}},
	volume = {12},
	journal = {the Journal of machine Learning research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and {others}},
	year = {2011},
	note = {Publisher: JMLR. org},
	pages = {2825--2830},
}

@book{salton_introduction_1986,
	address = {USA},
	title = {Introduction to {Modern} {Information} {Retrieval}},
	isbn = {0-07-054484-0},
	publisher = {McGraw-Hill, Inc.},
	author = {Salton, Gerard and McGill, Michael J.},
	year = {1986},
}

@article{gros_evolution_2009,
	title = {The evolution of epistasis and its links with genetic robustness, complexity and drift in a phenotypic model of adaptation},
	volume = {182},
	number = {1},
	journal = {Genetics},
	author = {Gros, Pierre-Alexis and Le Nagard, Hervé and Tenaillon, Olivier},
	year = {2009},
	note = {Publisher: Genetics Soc America},
	pages = {277--293},
}

@article{terpilowski_scikit-posthocs_2019,
	title = {scikit-posthocs: {Pairwise} multiple comparison tests in {Python}},
	volume = {4},
	url = {https://doi.org/10.21105/joss.01169},
	doi = {10.21105/joss.01169},
	number = {36},
	journal = {Journal of Open Source Software},
	author = {Terpilowski, Maksim A.},
	year = {2019},
	note = {Publisher: The Open Journal},
	pages = {1169},
}

@article{thomee_yfcc100m_2016,
	title = {{YFCC100M}: {The} new data in multimedia research},
	volume = {59},
	number = {2},
	journal = {Communications of the ACM},
	author = {Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
	year = {2016},
	note = {Publisher: ACM New York, NY, USA},
	pages = {64--73},
}

@article{zhu_recall_2004,
	title = {Recall, precision and average precision},
	volume = {2},
	journal = {Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
	author = {Zhu, Mu},
	year = {2004},
	pages = {30},
}

@article{nemenyi_distribution-free_1963,
	title = {Distribution-free multiple comparisons (doctoral dissertation, princeton university, 1963)},
	volume = {25},
	number = {2},
	journal = {Dissertation Abstracts International},
	author = {Nemenyi, PB},
	year = {1963},
	pages = {1233},
}

@inproceedings{prati_combining_2012,
	title = {Combining feature ranking algorithms through rank aggregation},
	doi = {10.1109/IJCNN.2012.6252467},
	abstract = {The problem of combining multiple feature rankings into a more robust ranking is investigated. A general framework for ensemble feature ranking is proposed, alongside four instantiations of this framework using different ranking aggregation methods. An empirical evaluation using 39 UCI datasets, three different learning algorithms and three different performance measures enable us to reach a compelling conclusion: ensemble feature ranking do improve the quality of feature rankings. Furthermore, one of the proposed methods was able to achieve results statistically significantly better than the others.},
	booktitle = {The 2012 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Prati, Ronaldo C.},
	month = jun,
	year = {2012},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}

@article{ghosh_interpretable_2020,
	title = {Interpretable {Artificial} {Intelligence}: {Why} and {When}},
	volume = {214},
	issn = {0361-803X},
	shorttitle = {Interpretable {Artificial} {Intelligence}},
	url = {https://www.ajronline.org/doi/full/10.2214/AJR.19.22145},
	doi = {10.2214/AJR.19.22145},
	abstract = {Choose
                    Top of pageABSTRACT {\textless}{\textless}Conclusion: The Way Forwa...References OBJECTIVE. The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. CONCLUSION. A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge.},
	number = {5},
	urldate = {2021-05-27},
	journal = {American Journal of Roentgenology},
	author = {Ghosh, Adarsh and Kandasamy, Devasenathipathy},
	month = mar,
	year = {2020},
	note = {Publisher: American Roentgen Ray Society},
	keywords = {biomedical research, deep learning, machine learning},
	pages = {1137--1138},
}

@inproceedings{duch_comparison_2004,
	title = {Comparison of feature ranking methods based on information entropy},
	volume = {2},
	doi = {10.1109/IJCNN.2004.1380157},
	abstract = {A comparison between five feature ranking methods based on entropy is presented on artificial and real datasets. Feature ranking method using /spl chi//sup 2/ statistics gives results that are very similar to the entropy-based methods. The quality of feature rankings obtained by these methods is evaluated using the decision tree and the nearest neighbor classifier with growing number of most important features. Significant differences are found in some cases, but there is no single best index that works best for all data and all classifiers. Therefore to be sure that a subset of features giving highest accuracy has been selected requires the use of many different indices.},
	booktitle = {2004 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks} ({IEEE} {Cat}. {No}.{04CH37541})},
	author = {Duch, W. and Wieczorek, T. and Biesiada, J. and Blachnik, M.},
	month = jul,
	year = {2004},
	note = {ISSN: 1098-7576},
	keywords = {Bioinformatics, Classification tree analysis, Decision trees, Feature extraction, Filters, Informatics, Information entropy, Nearest neighbor searches, Statistics, Text analysis},
	pages = {1415--1419 vol.2},
}

@article{britt_high-performance_2017,
	title = {High-{Performance} {Computing} with {Quantum} {Processing} {Units}},
	volume = {13},
	issn = {1550-4832},
	url = {https://doi.org/10.1145/3007651},
	doi = {10.1145/3007651},
	abstract = {The prospects of quantum computing have driven efforts to realize fully functional quantum processing units (QPUs). Recent success in developing proof-of-principle QPUs has prompted the question of how to integrate these emerging processors into modern high-performance computing (HPC) systems. We examine how QPUs can be integrated into current and future HPC system architectures by accounting for functional and physical design requirements. We identify two integration pathways that are differentiated by infrastructure constraints on the QPU and the use cases expected for the HPC system. This includes a tight integration that assumes infrastructure bottlenecks can be overcome as well as a loose integration that assumes they cannot. We find that the performance of both approaches is likely to depend on the quantum interconnect that serves to entangle multiple QPUs. We also identify several challenges in assessing QPU performance for HPC, and we consider new metrics that capture the interplay between system architecture and the quantum parallelism underlying computational performance.},
	number = {3},
	urldate = {2021-05-27},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Britt, Keith A. and Humble, Travis S.},
	month = mar,
	year = {2017},
	keywords = {Quantum computing, accelerator, high performance computing},
	pages = {39:1--39:13},
}

@inproceedings{sagiroglu_big_2013,
	title = {Big data: {A} review},
	shorttitle = {Big data},
	doi = {10.1109/CTS.2013.6567202},
	abstract = {Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.},
	booktitle = {2013 {International} {Conference} on {Collaboration} {Technologies} and {Systems} ({CTS})},
	author = {Sagiroglu, Seref and Sinanc, Duygu},
	month = may,
	year = {2013},
	keywords = {Data handling, Data models, Data storage systems, Information management, Organizations, Security, big data, value, variety, velocity, verification, volume},
	pages = {42--47},
}

@article{theis_end_2017,
	title = {The {End} of {Moore}'s {Law}: {A} {New} {Beginning} for {Information} {Technology}},
	volume = {19},
	issn = {1558-366X},
	shorttitle = {The {End} of {Moore}'s {Law}},
	doi = {10.1109/MCSE.2017.29},
	abstract = {The insights contained in Gordon Moore's now famous 1965 and 1975 papers have broadly guided the development of semiconductor electronics for over 50 years. However, the field-effect transistor is approaching some physical limits to further miniaturization, and the associated rising costs and reduced return on investment appear to be slowing the pace of development. Far from signaling an end to progress, this gradual "end of Moore's law" will open a new era in information technology as the focus of research and development shifts from miniaturization of long-established technologies to the coordinated introduction of new devices, new integration technologies, and new architectures for computing.},
	number = {2},
	journal = {Computing in Science Engineering},
	author = {Theis, Thomas N. and Wong, H.-S. Philip},
	month = mar,
	year = {2017},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {read 80\%},
	pages = {41--50},
}

@article{kish_end_2002,
	title = {End of {Moore}'s law: thermal (noise) death of integration in micro and nano electronics},
	volume = {305},
	issn = {0375-9601},
	shorttitle = {End of {Moore}'s law},
	url = {https://www.sciencedirect.com/science/article/pii/S0375960102013658},
	doi = {10.1016/S0375-9601(02)01365-8},
	abstract = {The exponential growth of memory size and clock frequency in computers has a great impact on everyday life. The growth is empirically described by Moore's law of miniaturization. Physical limitations of this growth would have a serious impact on technology and economy. A thermodynamical effect, the increasing thermal noise voltage (Johnson–Nyquist noise) on decreasing characteristic capacitances, together with the constrain of using lower supply voltages to keep power dissipation manageable on the contrary of increasing clock frequency, has the potential to break abruptly Moore's law within 6–8 years, or earlier.},
	language = {en},
	number = {3},
	urldate = {2021-05-19},
	journal = {Physics Letters A},
	author = {Kish, Laszlo B},
	month = dec,
	year = {2002},
	pages = {144--149},
}

@phdthesis{overschie_feature_2020,
	title = {Feature selection performance assessment},
	copyright = {All rights reserved},
	abstract = {The evaluation of feature selection algorithms is a many-faceted problem, which has been conducted in different ways in the literature. With a unified theoretical framework lacking, authors resort to widely used methods of evaluation - which are not necessarily optimal. In this paper, all relevant aspects of the evaluation process are thoroughly analyzed, after which a set of sensible metrics is distilled. Using a priori information about relevant features, promising metrics to measure feature subset quality include the ROCAUC- and mAP scores. Furthermore, measures of stability and statistical integrity testing are given. An implementation of the pipeline, fseval, is made publicly available. Finally, a quantitative experiment implements and discusses the newly proposed set of metrics, which shows the new metrics to be of value and able to foretell feature subset prediction performance.},
	school = {University of Groningen},
	author = {Overschie, Jeroen},
	month = jun,
	year = {2020},
	keywords = {benchmark},
}

@article{peng_feature_2005,
	title = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2005.159},
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Peng, Hanchuan and Long, Fuhui and Ding, C.},
	month = aug,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {to read},
	pages = {1226--1238},
}

@article{song_gene_2007,
	title = {Gene selection via the {BAHSIC} family of algorithms},
	volume = {23},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btm216},
	doi = {10.1093/bioinformatics/btm216},
	abstract = {Motivation: Identifying significant genes among thousands of sequences on a microarray is a central challenge for cancer research in bioinformatics. The ultimate goal is to detect the genes that are involved in disease outbreak and progression. A multitude of methods have been proposed for this task of feature selection, yet the selected gene lists differ greatly between different methods. To accomplish biologically meaningful gene selection from microarray data, we have to understand the theoretical connections and the differences between these methods. In this article, we define a kernel-based framework for feature selection based on the Hilbert–Schmidt independence criterion and backward elimination, called BAHSIC. We show that several well-known feature selectors are instances of BAHSIC, thereby clarifying their relationship. Furthermore, by choosing a different kernel, BAHSIC allows us to easily define novel feature selection algorithms. As a further advantage, feature selection via BAHSIC works directly on multiclass problems.Results: In a broad experimental evaluation, the members of the BAHSIC family reach high levels of accuracy and robustness when compared to other feature selection techniques. Experiments show that features selected with a linear kernel provide the best classification performance in general, but if strong non-linearities are present in the data then non-linear kernels can be more suitable.Availability: Accompanying homepage is http://www.dbs.ifi.lmu.de/{\textasciitilde}borgward/BAHSICContact:kb@dbs.ifi.lmu.deSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {13},
	urldate = {2021-04-24},
	journal = {Bioinformatics},
	author = {Song, Le and Bedo, Justin and Borgwardt, Karsten M. and Gretton, Arthur and Smola, Alex},
	month = jul,
	year = {2007},
	keywords = {to read},
	pages = {i490--i498},
}

@article{chen_kernel_2018,
	title = {Kernel {Feature} {Selection} via {Conditional} {Covariance} {Minimization}},
	url = {http://arxiv.org/abs/1707.01164},
	abstract = {We propose a method for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we show how to perform feature selection via a constrained optimization problem involving the trace of the conditional covariance operator. We prove various consistency results for this procedure, and also demonstrate that our method compares favorably with other state-of-the-art algorithms on a variety of synthetic and real data sets.},
	urldate = {2021-04-24},
	journal = {arXiv:1707.01164 [cs, stat]},
	author = {Chen, Jianbo and Stern, Mitchell and Wainwright, Martin J. and Jordan, Michael I.},
	month = oct,
	year = {2018},
	note = {arXiv: 1707.01164},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, to read},
}

@article{wojtas_feature_2020,
	title = {Feature {Importance} {Ranking} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2010.08973},
	abstract = {Feature importance ranking has become a powerful tool for explainable AI. However, its nature of combinatorial optimization poses a great challenge for deep learning. In this paper, we propose a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset of a fixed size and ranking the importance of those features in the optimal subset simultaneously. During learning, the operator is trained for a supervised learning task via optimal feature subset candidates generated by the selector that learns predicting the learning performance of the operator working on different optimal subset candidates. We develop an alternate learning algorithm that trains two nets jointly and incorporates a stochastic local search procedure into learning to address the combinatorial optimization challenge. In deployment, the selector generates an optimal feature subset and ranks feature importance, while the operator makes predictions based on the optimal subset for test data. A thorough evaluation on synthetic, benchmark and real data sets suggests that our approach outperforms several state-of-the-art feature importance ranking and supervised feature selection methods. (Our source code is available: https://github.com/maksym33/FeatureImportanceDL)},
	urldate = {2021-04-24},
	journal = {arXiv:2010.08973 [cs]},
	author = {Wojtas, Maksymilian and Chen, Ke},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08973},
	keywords = {to read},
}

@article{adamczewski_q-fit_2020,
	title = {Q-{FIT}: {The} {Quantifiable} {Feature} {Importance} {Technique} for {Explainable} {Machine} {Learning}},
	shorttitle = {Q-{FIT}},
	url = {http://arxiv.org/abs/2010.13872},
	abstract = {We introduce a novel framework to quantify the importance of each input feature for model explainability. A user of our framework can choose between two modes: (a) global explanation: providing feature importance globally across all the data points; and (b) local explanation: providing feature importance locally for each individual data point. The core idea of our method comes from utilizing the Dirichlet distribution to define a distribution over the importance of input features. This particular distribution is useful in ranking the importance of the input features as a sample from this distribution is a probability vector (i.e., the vector components sum to 1), Thus, the ranking uncovered by our framework which provides a {\textbackslash}textit\{quantifiable explanation\} of how significant each input feature is to a model's output. This quantifiable explainability differentiates our method from existing feature-selection methods, which simply determine whether a feature is relevant or not. Furthermore, a distribution over the explanation allows to define a closed-form divergence to measure the similarity between learned feature importance under different models. We use this divergence to study how the feature importance trade-offs with essential notions in modern machine learning, such as privacy and fairness. We show the effectiveness of our method on a variety of synthetic and real datasets, taking into account both tabular and image datasets.},
	urldate = {2021-04-24},
	journal = {arXiv:2010.13872 [cs, stat]},
	author = {Adamczewski, Kamil and Harder, Frederik and Park, Mijung},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.13872},
	keywords = {read 20\%, to read},
}

@inproceedings{yoon_invase_2019,
	title = {{INVASE}: {Instance}-wise {Variable} {Selection} using {Neural} {Networks}},
	shorttitle = {{INVASE}},
	abstract = {Semantic Scholar extracted view of "INVASE: Instance-wise Variable Selection using Neural Networks" by Jinsung Yoon et al.},
	booktitle = {{ICLR}},
	author = {Yoon, Jinsung and Jordon, James and Schaar, M.},
	year = {2019},
	keywords = {to read},
}

@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2021-04-22},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv: 1312.6034},
	keywords = {to read},
}

@article{ribeiro_model-agnostic_2016,
	title = {Model-{Agnostic} {Interpretability} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.05386},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
	urldate = {2021-04-22},
	journal = {arXiv:1606.05386 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05386},
	keywords = {to read},
}

@article{shrikumar_learning_2019,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
	urldate = {2021-04-22},
	journal = {arXiv:1704.02685 [cs]},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = oct,
	year = {2019},
	note = {arXiv: 1704.02685},
	keywords = {to read},
}

@article{gu_generalized_2012,
	title = {Generalized {Fisher} {Score} for {Feature} {Selection}},
	url = {http://arxiv.org/abs/1202.3725},
	abstract = {Fisher score is one of the most widely used supervised feature selection methods. However, it selects each feature independently according to their scores under the Fisher criterion, which leads to a suboptimal subset of features. In this paper, we present a generalized Fisher score to jointly select features. It aims at finding an subset of features, which maximize the lower bound of traditional Fisher score. The resulting feature selection problem is a mixed integer programming, which can be reformulated as a quadratically constrained linear programming (QCLP). It is solved by cutting plane algorithm, in each iteration of which a multiple kernel learning problem is solved alternatively by multivariate ridge regression and projected gradient descent. Experiments on benchmark data sets indicate that the proposed method outperforms Fisher score as well as many other state-of-the-art feature selection methods.},
	urldate = {2021-04-20},
	journal = {arXiv:1202.3725 [cs, stat]},
	author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.3725},
	keywords = {novel algorithm, read 20\%, read 40\%},
}

@article{zaffalon_robust_2014,
	title = {Robust {Feature} {Selection} by {Mutual} {Information} {Distributions}},
	url = {http://arxiv.org/abs/1408.1487},
	abstract = {Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.},
	urldate = {2021-04-20},
	journal = {arXiv:1408.1487 [cs]},
	author = {Zaffalon, Marco and Hutter, Marcus},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.1487},
	keywords = {novel algorithm, read 20\%},
}

@inproceedings{kononenko_estimating_1994,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Estimating attributes: {Analysis} and extensions of {RELIEF}},
	isbn = {978-3-540-48365-6},
	shorttitle = {Estimating attributes},
	doi = {10.1007/3-540-57868-4_57},
	abstract = {In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML}-94},
	publisher = {Springer},
	author = {Kononenko, Igor},
	editor = {Bergadano, Francesco and De Raedt, Luc},
	year = {1994},
	keywords = {novel algorithm, read 20\%, read 40\%},
	pages = {171--182},
}

@article{chen_learning_2018,
	title = {Learning to {Explain}: {An} {Information}-{Theoretic} {Perspective} on {Model} {Interpretation}},
	shorttitle = {Learning to {Explain}},
	url = {http://arxiv.org/abs/1802.07814},
	abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
	urldate = {2021-04-20},
	journal = {arXiv:1802.07814 [cs, stat]},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.07814},
	keywords = {novel algorithm, read 20\%, read 40\%, synthetic datasets, to read},
}

@article{greenwell_simple_2018,
	title = {A {Simple} and {Effective} {Model}-{Based} {Variable} {Importance} {Measure}},
	url = {http://arxiv.org/abs/1805.04755},
	abstract = {In the era of "big data", it is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. For example, it is often of interest to know which, if any, of the predictors in a fitted model are relatively influential on the predicted outcome. Some modern algorithms---like random forests and gradient boosted decision trees---have a natural way of quantifying the importance or relative influence of each feature. Other algorithms---like naive Bayes classifiers and support vector machines---are not capable of doing so and model-free approaches are generally used to measure each predictor's importance. In this paper, we propose a standardized, model-based approach to measuring predictor importance across the growing spectrum of supervised learning algorithms. Our proposed method is illustrated through both simulated and real data examples. The R code to reproduce all of the figures in this paper is available in the supplementary materials.},
	urldate = {2020-09-25},
	journal = {arXiv:1805.04755 [cs, stat]},
	author = {Greenwell, Brandon M. and Boehmke, Bradley C. and McCarthy, Andrew J.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04755},
	keywords = {metrics, read 20\%, read 40\%},
}

@article{zeng_novel_2015,
	title = {A novel feature selection method considering feature interaction},
	volume = {48},
	issn = {0031-3203},
	url = {https://doi.org/10.1016/j.patcog.2015.02.025},
	doi = {10.1016/j.patcog.2015.02.025},
	abstract = {Interacting features are those that appear to be irrelevant or weakly relevant with the class individually, but when it combined with other features, it may highly correlate to the class. Discovering feature interaction is a challenging task in feature selection. In this paper, a novel feature selection algorithm considering feature interaction is proposed. Firstly, feature relevance, feature redundancy and feature interaction have been redefined in the framework of information theory. Then the interaction weight factor which can reflect the information of whether a feature is redundant or interactive is proposed. Afterwards, we bring forward an Interaction Weight based Feature Selection algorithm (IWFS). To evaluate the performance of the proposed algorithm, we compare IWFS with other five representative feature selection algorithms, including CFS, INTERACT, FCBF, MRMR and Relief-F, in terms of the classification accuracies and the number of selected features with three different types of classifiers including C4.5, IB1 and PART. The results on the six synthetic datasets show that IWFS can effectively identify irrelevant and redundant features while reserving interactive ones. The results on the eight real world datasets indicate that IWFS not only efficiently reduces the dimensionality of feature space, but also offers the highest average accuracy for all the three classification algorithms. A novel feature selection method based on interaction weight factor is proposed.We redefined relevance, redundancy and interaction of features in the framework of information theory.The algorithm can deal with irrelevant, redundant and interactive features.Our method obtains the best average accuracies compared with the other five algorithms.},
	number = {8},
	urldate = {2020-09-23},
	journal = {Pattern Recognition},
	author = {Zeng, Zilin and Zhang, Hongjun and Zhang, Rui and Yin, Chengxiang},
	month = aug,
	year = {2015},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {2656--2666},
}

@article{bennasar_feature_2015,
	title = {Feature selection using {Joint} {Mutual} {Information} {Maximisation}},
	volume = {42},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417415004674},
	doi = {10.1016/j.eswa.2015.07.007},
	abstract = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the ‘maximum of the minimum’ criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6\% in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
	language = {en},
	number = {22},
	urldate = {2020-09-23},
	journal = {Expert Systems with Applications},
	author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
	month = dec,
	year = {2015},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {8520--8532},
}

@inproceedings{zhao_searching_2007,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'07},
	title = {Searching for interacting features},
	url = {https://www.semanticscholar.org/paper/Searching-for-Interacting-Features-Zhao-Liu/d2debe138a9b67d838b11d622651383322934aee},
	abstract = {Feature interaction presents a challenge to feature selection for classification. A feature by itself may have little correlation with the target concept, but when it is combined with some other features, they can be strongly correlated with the target concept. Unintentional removal of these features can result in poor classification performance. Handling feature interaction can be computationally intractable. Recognizing the presence of feature interaction, we propose to efficiently handle feature interaction to achieve efficient feature selection and present extensive experimental results of evaluation.},
	urldate = {2020-10-21},
	booktitle = {Proceedings of the 20th international joint conference on {Artifical} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Zhao, Zheng and Liu, Huan},
	month = jan,
	year = {2007},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {1156--1161},
}

@article{wang_feature_2007,
	title = {Feature selection based on rough sets and particle swarm optimization},
	volume = {28},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865506002327},
	doi = {10.1016/j.patrec.2006.09.003},
	abstract = {We propose a new feature selection strategy based on rough sets and particle swarm optimization (PSO). Rough sets have been used as a feature selection method with much success, but current hill-climbing rough set approaches to feature selection are inadequate at finding optimal reductions as no perfect heuristic can guarantee optimality. On the other hand, complete searches are not feasible for even medium-sized datasets. So, stochastic approaches provide a promising feature selection mechanism. Like Genetic Algorithms, PSO is a new evolutionary computation technique, in which each potential solution is seen as a particle with a certain velocity flying through the problem space. The Particle Swarms find optimal regions of the complex search space through the interaction of individuals in the population. PSO is attractive for feature selection in that particle swarms will discover best feature combinations as they fly within the subset space. Compared with GAs, PSO does not need complex operators such as crossover and mutation, it requires only primitive and simple mathematical operators, and is computationally inexpensive in terms of both memory and runtime. Experimentation is carried out, using UCI data, which compares the proposed algorithm with a GA-based approach and other deterministic rough set reduction algorithms. The results show that PSO is efficient for rough set-based feature selection.},
	language = {en},
	number = {4},
	urldate = {2020-09-23},
	journal = {Pattern Recognition Letters},
	author = {Wang, Xiangyang and Yang, Jie and Teng, Xiaolong and Xia, Weijun and Jensen, Richard},
	month = mar,
	year = {2007},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {459--471},
}

@article{friedman_predictive_2008,
	title = {Predictive learning via rule ensembles},
	volume = {2},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0811.1679},
	doi = {10.1214/07-AOAS148},
	abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
	number = {3},
	urldate = {2020-09-25},
	journal = {The Annals of Applied Statistics},
	author = {Friedman, Jerome H. and Popescu, Bogdan E.},
	month = sep,
	year = {2008},
	note = {arXiv: 0811.1679},
	keywords = {no-label-yet, read 20\%},
	pages = {916--954},
}

@article{bommert_stabm_nodate,
	title = {stabm: {Stability} {Measures} for {Feature} {Selection}},
	abstract = {The R (R Core Team, 2020) package stabm provides functionality for quantifying the similarity of two or more sets. For example, consider the two sets \{A, B, C, D\} and \{A, B, C, E\}. Intuitively, these sets are quite similar because their overlap is large compared to the cardinality of the two sets. The R package stabm implements functions to express the similarity of sets by a real valued score. Quantifying the similarity of sets is useful for comparing sets of selected features. But also for many other tasks like similarity analyses of gene sets or text corpora, the R package stabm can be employed.},
	language = {en},
	author = {Bommert, Andrea and Lang, Michel},
	keywords = {read 20\%, stability},
	pages = {4},
}

@article{oh_feature_2019,
	title = {Feature {Interaction} in {Terms} of {Prediction} {Performance}},
	volume = {9},
	url = {https://www.researchgate.net/publication/337645592_Feature_Interaction_in_Terms_of_Prediction_Performance},
	doi = {10.3390/app9235191},
	abstract = {There has been considerable development in machine learning in recent years with some remarkable successes. Although there are many high-performance methods, the interpretation of learning models remains challenging. Understanding the underlying theory behind the specific prediction of various models is difficult. Various studies have attempted to explain the working principle behind learning models using techniques like feature importance, partial dependency, feature interaction, and the Shapley value. This study introduces a new feature interaction measure. While recent studies have measured feature interaction using partial dependency, this study redefines feature interaction in terms of prediction performance. The proposed measure is easy to interpret, faster than partial dependency-based measures, and useful to explain feature interaction, which affects prediction performance in both regression and classification models.},
	journal = {Applied Sciences},
	author = {Oh, Sejong},
	month = nov,
	year = {2019},
	keywords = {metrics, read 20\%},
	pages = {5191},
}

@misc{noauthor_discovering_nodate,
	title = {Discovering {Interaction} {Effects} in {Ensemble} {Models}},
	url = {https://blog.macuyiko.com/post/2019/discovering-interaction-effects-in-ensemble-models.html},
	urldate = {2020-10-12},
	keywords = {no-label-yet},
}

@article{zhao_searching_2009,
	title = {Searching for interacting features in subset selection},
	volume = {13},
	issn = {1088-467X},
	url = {https://content.iospress.com/articles/intelligent-data-analysis/ida00364},
	doi = {10.3233/IDA-2009-0364},
	abstract = {The evolving and adapting capabilities of robust intelligence are best manifested in its ability to learn. Machine learning enables computer systems to learn, and improve performance. Feature selection facilitates machine learning (e.g., classificati},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Intelligent Data Analysis},
	author = {Zhao, Zheng and Liu, Huan},
	month = jan,
	year = {2009},
	keywords = {novel algorithm, read 20\%, read 40\%},
	pages = {207--228},
}

@article{goyal_feature_2020,
	title = {Feature {Interactions} in {XGBoost}},
	url = {http://arxiv.org/abs/2007.05758},
	abstract = {In this paper, we investigate how feature interactions can be identified to be used as constraints in the gradient boosting tree models using XGBoost's implementation. Our results show that accurate identification of these constraints can help improve the performance of baseline XGBoost model significantly. Further, the improvement in the model structure can also lead to better interpretability.},
	urldate = {2020-09-23},
	journal = {arXiv:2007.05758 [cs, stat]},
	author = {Goyal, Kshitij and Dumancic, Sebastijan and Blockeel, Hendrik},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.05758},
	keywords = {read 20\%, read 40\%, synthetic datasets},
}

@book{molnar_54_nodate,
	title = {5.4 {Feature} {Interaction} {\textbar} {Interpretable} {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/interaction.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2020-09-23},
	author = {Molnar, Christoph},
	keywords = {read 20\%, read 40\%, synthetic datasets},
}

@article{dash_feature_1997,
	title = {Feature {Selection} for {Classification}},
	doi = {10.1016/S1088-467X(97)00008-5},
	abstract = {Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.},
	journal = {Intell. Data Anal.},
	author = {Dash, M. and Liu, Huan},
	year = {1997},
	keywords = {benchmark, read 20\%, read 40\%},
}

@inproceedings{jovic_review_2015,
	title = {A review of feature selection methods with applications},
	doi = {10.1109/MIPRO.2015.7160458},
	abstract = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
	booktitle = {2015 38th {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	author = {Jović, A. and Brkić, K. and Bogunović, N.},
	month = may,
	year = {2015},
	keywords = {benchmark, read 20\%, read 40\%},
	pages = {1200--1205},
}

@inproceedings{okimoto_complexity_2017,
	title = {Complexity {Measures} {Effectiveness} in {Feature} {Selection}},
	doi = {10.1109/BRACIS.2017.66},
	abstract = {Feature selection is an important pre-processing step usually mandatory in data analysis by Machine Learning techniques. Its objective is to reduce data dimensionality by removing irrelevant and redundant features from a dataset. In this work we investigate how the presence of irrelevant features in a dataset affects the complexity of a classification problem solution. This is performed by monitoring the values of some complexity measures extracted from the original and preprocessed datasets. These descriptors allow estimating the intrinsic difficulty of a classification problem. Some of these measures are then used in feature ranking. The results are promising and reveal that the complexity measures are indeed suitable for estimating feature importance in classification datasets.},
	booktitle = {2017 {Brazilian} {Conference} on {Intelligent} {Systems} ({BRACIS})},
	author = {Okimoto, Lucas Chesini and Savii, Ricardo Manhães and Lorena, Ana Carolina},
	month = oct,
	year = {2017},
	keywords = {read 20\%, synthetic datasets},
	pages = {91--96},
}

@article{rivolli_characterizing_2019,
	title = {Characterizing classification datasets: a study of meta-features for meta-learning},
	shorttitle = {Characterizing classification datasets},
	url = {http://arxiv.org/abs/1808.10406},
	abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
	urldate = {2020-10-08},
	journal = {arXiv:1808.10406 [cs, stat]},
	author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
	month = aug,
	year = {2019},
	note = {arXiv: 1808.10406
version: 2},
	keywords = {meta-learning, read 20\%},
}

@article{alcobaca_mfe_2020,
	title = {{MFE}: {Towards} reproducible meta-feature extraction},
	volume = {21},
	shorttitle = {{MFE}},
	url = {https://jmlr.org/papers/v21/19-348.html},
	abstract = {Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.},
	journal = {Journal of Machine Learning Research},
	author = {Alcobaça, Edesio and Siqueira, Felipe and Garcia, Luís Paulo and Rivolli, Adriano and Oliva, Jefferson and de Carvalho, Andre},
	month = jan,
	year = {2020},
	keywords = {meta-learning, read 20\%, read 40\%},
	pages = {1--5},
}

@article{pinto_autobagging_2017,
	title = {{autoBagging}: {Learning} to {Rank} {Bagging} {Workflows} with {Metalearning}},
	shorttitle = {{autoBagging}},
	url = {http://arxiv.org/abs/1706.09367},
	abstract = {Machine Learning (ML) has been successfully applied to a wide range of domains and applications. One of the techniques behind most of these successful applications is Ensemble Learning (EL), the field of ML that gave birth to methods such as Random Forests or Boosting. The complexity of applying these techniques together with the market scarcity on ML experts, has created the need for systems that enable a fast and easy drop-in replacement for ML libraries. Automated machine learning (autoML) is the field of ML that attempts to answers these needs. Typically, these systems rely on optimization techniques such as bayesian optimization to lead the search for the best model. Our approach differs from these systems by making use of the most recent advances on metalearning and a learning to rank approach to learn from metadata. We propose autoBagging, an autoML system that automatically ranks 63 bagging workflows by exploiting past performance and dataset characterization. Results on 140 classification datasets from the OpenML platform show that autoBagging can yield better performance than the Average Rank method and achieve results that are not statistically different from an ideal model that systematically selects the best workflow for each dataset. For the purpose of reproducibility and generalizability, autoBagging is publicly available as an R package on CRAN.},
	urldate = {2020-10-06},
	journal = {arXiv:1706.09367 [cs, stat]},
	author = {Pinto, Fábio and Cerqueira, Vítor and Soares, Carlos and Mendes-Moreira, João},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09367},
	keywords = {meta-learning, read 20\%},
}

@article{vanschoren_meta-learning_2018,
	title = {Meta-{Learning}: {A} {Survey}},
	shorttitle = {Meta-{Learning}},
	url = {http://arxiv.org/abs/1810.03548},
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
	urldate = {2020-09-29},
	journal = {arXiv:1810.03548 [cs, stat]},
	author = {Vanschoren, Joaquin},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03548},
	keywords = {meta-learning, read 20\%, read 40\%, read 60\%},
}

@misc{derksen_visualising_2019,
	title = {Visualising high-dimensional datasets using {PCA} and t-{SNE} in {Python}},
	url = {https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b},
	abstract = {Update: April 29, 2019. Updated some of the code to not use ggplot but instead use seaborn and matplotlib. I also added an example for a…},
	language = {en},
	urldate = {2020-10-12},
	journal = {Medium},
	author = {Derksen, Luuk},
	month = apr,
	year = {2019},
	keywords = {dimensionality reduction},
}

@misc{noauthor_22_nodate,
	title = {2.2. {Manifold} learning — scikit-learn 0.23.2 documentation},
	url = {https://scikit-learn.org/stable/modules/manifold.html#manifold},
	urldate = {2020-10-12},
	keywords = {dimensionality reduction},
}

@article{chapman_dataset_2020,
	title = {Dataset search: a survey},
	volume = {29},
	issn = {0949-877X},
	shorttitle = {Dataset search},
	url = {https://doi.org/10.1007/s00778-019-00564-x},
	doi = {10.1007/s00778-019-00564-x},
	abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
	language = {en},
	number = {1},
	urldate = {2020-09-23},
	journal = {The VLDB Journal},
	author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ibáñez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
	month = jan,
	year = {2020},
	keywords = {synthetic datasets},
	pages = {251--272},
}

@inproceedings{guzman-martinez_feature_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Feature {Selection} {Stability} {Assessment} {Based} on the {Jensen}-{Shannon} {Divergence}},
	isbn = {978-3-642-23780-5},
	doi = {10.1007/978-3-642-23780-5_48},
	abstract = {Feature selection and ranking techniques play an important role in the analysis of high-dimensional data. In particular, their stability becomes crucial when the feature importance is later studied in order to better understand the underlying process. The fact that a small change in the dataset may affect the outcome of the feature selection/ranking algorithm has been long overlooked in the literature. We propose an information-theoretic approach, using the Jensen-Shannon divergence to assess this stability (or robustness). Unlike other measures, this new metric is suitable for different algorithm outcomes: full ranked lists, partial sublists (top-k lists) as well as the least studied partial ranked lists. This generalized metric attempts to measure the disagreement among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the differences that appear at the top of the list. We illustrate and compare it with popular metrics like the Spearman rank correlation and the Kuncheva’s index on feature selection/ranking outcomes artificially generated and on an spectral fat dataset with different filter-based feature selectors.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Guzmán-Martínez, Roberto and Alaiz-Rodríguez, Rocío},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	keywords = {read 20\%, stability},
	pages = {597--612},
}

@article{xu_synthesizing_2018,
	title = {Synthesizing {Tabular} {Data} using {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1811.11264},
	abstract = {Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.},
	urldate = {2021-04-19},
	journal = {arXiv:1811.11264 [cs, stat]},
	author = {Xu, Lei and Veeramachaneni, Kalyan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.11264},
	keywords = {synthetic datasets},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/start},
	urldate = {2020-10-21},
}

@inproceedings{parker_analysis_2011,
	title = {An {Analysis} of {Performance} {Measures} for {Binary} {Classifiers}},
	url = {https://ieeexplore.ieee.org/document/6137256},
	doi = {10.1109/ICDM.2011.21},
	abstract = {If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining if one classifier is better than another, given the same test data. Five of these are long established and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate, and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly towards one of the newer methods.},
	booktitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining}},
	author = {Parker, Charles},
	month = dec,
	year = {2011},
	note = {ISSN: 2374-8486},
	keywords = {metrics},
	pages = {517--526},
}

@inproceedings{john_irrelevant_1994,
	title = {Irrelevant {Features} and the {Subset} {Selection} {Problem}},
	doi = {10.1016/b978-1-55860-335-6.50023-4},
	abstract = {We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.},
	booktitle = {{ICML}},
	author = {John, George H. and Kohavi, R. and Pfleger, Karl},
	year = {1994},
	keywords = {benchmark, read 20\%},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	number = {86},
	urldate = {2020-10-12},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	keywords = {dimensionality reduction, read 20\%},
	pages = {2579--2605},
}

@article{wattenberg_how_2016,
	title = {How to {Use} t-{SNE} {Effectively}},
	volume = {1},
	issn = {2476-0757},
	url = {http://distill.pub/2016/misread-tsne},
	doi = {10.23915/distill.00002},
	abstract = {Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.},
	language = {en},
	number = {10},
	urldate = {2020-10-12},
	journal = {Distill},
	author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
	month = oct,
	year = {2016},
	keywords = {dimensionality reduction, read 20\%, read 40\%},
	pages = {e2},
}

@article{reis_featsel_2017,
	title = {featsel: {A} framework for benchmarking of feature selection algorithms and cost functions},
	volume = {6},
	issn = {2352-7110},
	shorttitle = {featsel},
	url = {http://www.sciencedirect.com/science/article/pii/S2352711017300286},
	doi = {10.1016/j.softx.2017.07.005},
	abstract = {In this paper, we introduce featsel, a framework for benchmarking of feature selection algorithms and cost functions. This framework allows the user to deal with the search space as a Boolean lattice and has its core coded in C++ for computational efficiency purposes. Moreover, featsel includes Perl scripts to add new algorithms and/or cost functions, generate random instances, plot graphs and organize results into tables. Besides, this framework already comes with dozens of algorithms and cost functions for benchmarking experiments. We also provide illustrative examples, in which featsel outperforms the popular Weka workbench in feature selection procedures on data sets from the UCI Machine Learning Repository.},
	language = {en},
	urldate = {2020-09-23},
	journal = {SoftwareX},
	author = {Reis, Marcelo S. and Estrela, Gustavo and Ferreira, Carlos Eduardo and Barrera, Junior},
	month = jan,
	year = {2017},
	keywords = {benchmark, read 20\%},
	pages = {193--197},
}

@article{parker_measuring_2013,
	title = {On measuring the performance of binary classifiers},
	volume = {35},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-012-0558-x},
	doi = {10.1007/s10115-012-0558-x},
	abstract = {If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining whether one classifier is better than another, given the same test data. Five of these are long established, and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly toward one of the newer methods.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Knowledge and Information Systems},
	author = {Parker, Charles},
	month = apr,
	year = {2013},
	keywords = {metrics},
	pages = {131--152},
}

@inproceedings{flach_coherent_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {A coherent interpretation of {AUC} as a measure of aggregated classification performance},
	isbn = {978-1-4503-0619-5},
	abstract = {The area under the ROC curve (AUC), a well-known measure of ranking performance, is also often used as a measure of classification performance, aggregating over decision thresholds as well as class and cost skews. However, David Hand has recently argued that AUC is fundamentally incoherent as a measure of aggregated classifier performance and proposed an alternative measure (Hand, 2009). Specifically, Hand derives a linear relationship between AUC and expected minimum loss, where the expectation is taken over a distribution of the misclassification cost parameter that depends on the model under consideration. Replacing this distribution with a Beta(2,2) distribution, Hand derives his alternative measure H. In this paper we offer an alternative, coherent interpretation of AUC as linearly related to expected loss. We use a distribution over cost parameter and a distribution over data points, both uniform and hence model-independent. Should one wish to consider only optimal thresholds, we demonstrate that a simple and more intuitive alternative to Hand's H measure is already available in the form of the area under the cost curve.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Flach, Peter and Hernández-Orallo, José and Ferri, Cèsar},
	month = jun,
	year = {2011},
	keywords = {metrics},
	pages = {657--664},
}

@article{hand_measuring_2009,
	title = {Measuring classifier performance: a coherent alternative to the area under the {ROC} curve},
	volume = {77},
	issn = {1573-0565},
	shorttitle = {Measuring classifier performance},
	url = {https://doi.org/10.1007/s10994-009-5119-5},
	doi = {10.1007/s10994-009-5119-5},
	abstract = {The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Machine Learning},
	author = {Hand, David J.},
	month = oct,
	year = {2009},
	keywords = {metrics},
	pages = {103--123},
}

@misc{ahemad_selecting_2019,
	title = {Selecting the {Right} {Metric} for {Skewed} {Classification} {Problems}},
	url = {https://towardsdatascience.com/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7},
	abstract = {Skew Accuracy!! Lets try few other classification metrics!!},
	language = {en},
	urldate = {2020-10-09},
	journal = {Medium},
	author = {Ahemad, Faizan},
	month = mar,
	year = {2019},
	keywords = {metrics, read 20\%, read 40\%},
}

@inproceedings{katz_explorekit_2016,
	address = {Barcelona, Spain},
	title = {{ExploreKit}: {Automatic} {Feature} {Generation} and {Selection}},
	isbn = {9781509054732},
	shorttitle = {{ExploreKit}},
	url = {http://ieeexplore.ieee.org/document/7837936/},
	doi = {10.1109/ICDM.2016.0123},
	urldate = {2020-10-08},
	booktitle = {2016 {IEEE} 16th {International} {Conference} on {Data} {Mining} ({ICDM})},
	publisher = {IEEE},
	author = {Katz, Gilad and Shin, Eui Chul Richard and Song, Dawn},
	month = dec,
	year = {2016},
	keywords = {read 20\%},
	pages = {979--984},
}

@inproceedings{le_using_2017,
	title = {Using synthetic data to train neural networks is model-based reasoning},
	url = {https://arxiv.org/pdf/1703.00868.pdf},
	doi = {10.1109/IJCNN.2017.7966298},
	abstract = {We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Le, Tuan Anh and Baydin, Atilim Giineş and Zinkov, Robert and Wood, Frank},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {read 20\%, synthetic datasets},
	pages = {3514--3521},
}

@article{tripathi_learning_2019,
	title = {Learning to {Generate} {Synthetic} {Data} via {Compositing}},
	url = {http://arxiv.org/abs/1904.05475},
	abstract = {We present a task-aware approach to synthetic data generation. Our framework employs a trainable synthesizer network that is optimized to produce meaningful training samples by assessing the strengths and weaknesses of a `target' network. The synthesizer and target networks are trained in an adversarial manner wherein each network is updated with a goal to outdo the other. Additionally, we ensure the synthesizer generates realistic data by pairing it with a discriminator trained on real-world images. Further, to make the target classifier invariant to blending artefacts, we introduce these artefacts to background regions of the training images so the target does not over-fit to them. We demonstrate the efficacy of our approach by applying it to different target networks including a classification network on AffNIST, and two object detection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST benchmark, our approach is able to surpass the baseline results with just half the training examples. On the VOC person detection benchmark, we show improvements of up to 2.7\% as a result of our data augmentation. Similarly on the GMU detection benchmark, we report a performance boost of 3.5\% in mAP over the baseline method, outperforming the previous state of the art approaches by up to 7.5\% on specific categories.},
	urldate = {2020-10-08},
	journal = {arXiv:1904.05475 [cs]},
	author = {Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and Tyagi, Ambrish and Rehg, James M. and Chari, Visesh},
	month = jul,
	year = {2019},
	note = {arXiv: 1904.05475},
	keywords = {read 20\%, synthetic datasets},
}

@incollection{vanschoren_meta-learning_2019,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Meta-{Learning}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_2},
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Vanschoren, Joaquin},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_2},
	keywords = {meta-learning, read 20\%},
	pages = {35--61},
}

@article{triantafillou_meta-dataset_2020,
	title = {Meta-{Dataset}: {A} {Dataset} of {Datasets} for {Learning} to {Learn} from {Few} {Examples}},
	shorttitle = {Meta-{Dataset}},
	url = {http://arxiv.org/abs/1903.03096},
	abstract = {Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.},
	urldate = {2020-09-29},
	journal = {arXiv:1903.03096 [cs, stat]},
	author = {Triantafillou, Eleni and Zhu, Tyler and Dumoulin, Vincent and Lamblin, Pascal and Evci, Utku and Xu, Kelvin and Goroshin, Ross and Gelada, Carles and Swersky, Kevin and Manzagol, Pierre-Antoine and Larochelle, Hugo},
	month = apr,
	year = {2020},
	note = {arXiv: 1903.03096},
	keywords = {meta-learning, read 20\%},
}

@article{bolon-canedo_review_2013,
	title = {A review of feature selection methods on synthetic data},
	volume = {34},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-012-0487-8},
	doi = {10.1007/s10115-012-0487-8},
	abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
	language = {en},
	number = {3},
	urldate = {2020-09-25},
	journal = {Knowledge and Information Systems},
	author = {Bolón-Canedo, Verónica and Sánchez-Maroño, Noelia and Alonso-Betanzos, Amparo},
	month = mar,
	year = {2013},
	keywords = {benchmark, read 20\%, read 40\%, read 60\%, synthetic datasets},
	pages = {483--519},
}

@misc{klein_machine_nodate,
	title = {Machine {Learning} with {Python}: {Create} {Artificial} {Datasets} for {Machine} {Learning} {Usage} with {Scikit}-{Learn}},
	url = {https://www.python-course.eu/machine_learning_create_datasets.php},
	urldate = {2020-09-23},
	author = {Klein, Bernd},
	keywords = {synthetic datasets},
}

@misc{vu_scikit-learn_nodate,
	title = {Scikit-{Learn} \& {More} for {Synthetic} {Dataset} {Generation} for {Machine} {Learning}},
	url = {https://www.kdnuggets.com/scikit-learn-more-for-synthetic-dataset-generation-for-machine-learning.html/},
	abstract = {While mature algorithms and extensive open-source libraries are widely available for machine learning practitioners, sufficient data to apply these techniques remains a core challenge. Discover how to leverage scikit-learn and other tools to generate synthetic data appropriate for optimizing and fine-tuning your models.},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {KDnuggets},
	author = {Vu, Kevin},
	keywords = {read 100\%, read 40\%, read 60\%, read 80\%, synthetic datasets},
}

@misc{weng_meta-learning_2018,
	title = {Meta-{Learning}: {Learning} to {Learn} {Fast}},
	shorttitle = {Meta-{Learning}},
	url = {https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html},
	abstract = {Meta-learning, also known as “learning to learn”, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 1) learn an efficient distance metric (metric-based); 2) use (recurrent) network with external or internal memory (model-based); 3)...},
	language = {en},
	urldate = {2020-09-23},
	journal = {Lil'Log},
	author = {Weng, Lilian},
	month = nov,
	year = {2018},
	keywords = {meta-learning},
}

@inproceedings{patki_synthetic_2016,
	title = {The {Synthetic} {Data} {Vault}},
	doi = {10.1109/DSAA.2016.49},
	abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	month = oct,
	year = {2016},
	keywords = {read 20\%, synthetic datasets},
	pages = {399--410},
}

@inproceedings{luxburg_clustering_2012,
	title = {Clustering: {Science} or {Art}?},
	shorttitle = {Clustering},
	url = {http://proceedings.mlr.press/v27/luxburg12a.html},
	abstract = {We examine whether the quality of different clustering algorithms can be compared by a general, scientifically sound procedure which is independent of particular clustering algorithms. We argue tha...},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Luxburg, Ulrike von and Williamson, Robert C. and Guyon, Isabelle},
	month = jun,
	year = {2012},
	keywords = {read 20\%, synthetic datasets},
	pages = {65--79},
}

@article{franti_k-means_2018,
	title = {K-means properties on six clustering benchmark datasets},
	volume = {48},
	issn = {0924-669X},
	url = {https://link.springer.com/epdf/10.1007/s10489-018-1238-7},
	doi = {10.1007/s10489-018-1238-7},
	abstract = {This paper has two contributions. First, we introduce a clustering basic benchmark. Second, we study the performance of k-means using this benchmark. Specifically, we measure how the performance depends on four factors: (1) overlap of clusters, (2) number of clusters, (3) dimensionality, and (4) unbalance of cluster sizes. The results show that overlap is critical, and that k-means starts to work effectively when the overlap reaches 4\% level.},
	language = {en},
	number = {12},
	urldate = {2020-09-24},
	journal = {Applied Intelligence},
	author = {Fränti, Pasi and Sieranoja, Sami},
	year = {2018},
	keywords = {read 20\%, read 40\%, synthetic datasets},
}

@inproceedings{bouneffouf_sampling_2015,
	title = {Sampling with {Minimum} {Sum} of {Squared} {Similarities} for {Nystrom}-{Based} {Large} {Scale} {Spectral} {Clustering}},
	abstract = {The Nystrom sampling provides an efficient approach for large scale clustering problems, by generating a low-rank matrix approximation. However, existing sampling methods are limited by their accuracies and computing times. This paper proposes a scalable Nystrom-based clustering algorithm with a new sampling procedure, Minimum Sum of Squared Similarities (MSSS). Here we provide a theoretical analysis of the upper error bound of our algorithm, and demonstrate its performance in comparison to the leading spectral clustering methods that use Nystrom sampling.},
	booktitle = {{IJCAI}},
	author = {Bouneffouf, Djallel and Birol, I.},
	year = {2015},
	keywords = {read 20\%, synthetic datasets},
}

@misc{sarkar_synthetic_2019,
	title = {Synthetic data generation — a must-have skill for new data scientists},
	url = {https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae},
	abstract = {A brief rundown of packages and ideas to generate synthetic data for self-driven data science projects and deep diving into machine…},
	language = {en},
	urldate = {2020-09-24},
	journal = {Medium},
	author = {Sarkar, Tirthajyoti},
	month = jul,
	year = {2019},
	keywords = {read 20\%, read 40\%, read 60\%, synthetic datasets},
}

@book{molnar_51_nodate,
	title = {5.1 {Partial} {Dependence} {Plot} ({PDP}) {\textbar} {Interpretable} {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/pdp.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2020-09-23},
	author = {Molnar, Christoph},
	keywords = {read 20\%, synthetic datasets},
}

@article{urbanowicz_gametes_2012,
	title = {{GAMETES}: a fast, direct algorithm for generating pure, strict, epistatic models with random architectures},
	volume = {5},
	issn = {1756-0381},
	shorttitle = {{GAMETES}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3605108/},
	doi = {10.1186/1756-0381-5-16},
	abstract = {Background
Geneticists who look beyond single locus disease associations require additional strategies for the detection of complex multi-locus effects. Epistasis, a multi-locus masking effect, presents a particular challenge, and has been the target of bioinformatic development. Thorough evaluation of new algorithms calls for simulation studies in which known disease models are sought. To date, the best methods for generating simulated multi-locus epistatic models rely on genetic algorithms. However, such methods are computationally expensive, difficult to adapt to multiple objectives, and unlikely to yield models with a precise form of epistasis which we refer to as pure and strict. Purely and strictly epistatic models constitute the worst-case in terms of detecting disease associations, since such associations may only be observed if all n-loci are included in the disease model. This makes them an attractive gold standard for simulation studies considering complex multi-locus effects.

Results
We introduce GAMETES, a user-friendly software package and algorithm which generates complex biallelic single nucleotide polymorphism (SNP) disease models for simulation studies. GAMETES rapidly and precisely generates random, pure, strict n-locus models with specified genetic constraints. These constraints include heritability, minor allele frequencies of the SNPs, and population prevalence. GAMETES also includes a simple dataset simulation strategy which may be utilized to rapidly generate an archive of simulated datasets for given genetic models. We highlight the utility and limitations of GAMETES with an example simulation study using MDR, an algorithm designed to detect epistasis.

Conclusions
GAMETES is a fast, flexible, and precise tool for generating complex n-locus models with random architectures. While GAMETES has a limited ability to generate models with higher heritabilities, it is proficient at generating the lower heritability models typically used in simulation studies evaluating new algorithms. In addition, the GAMETES modeling strategy may be flexibly combined with any dataset simulation strategy. Beyond dataset simulation, GAMETES could be employed to pursue theoretical characterization of genetic models and epistasis.},
	urldate = {2020-09-23},
	journal = {BioData Mining},
	author = {Urbanowicz, Ryan J and Kiralis, Jeff and Sinnott-Armstrong, Nicholas A and Heberling, Tamra and Fisher, Jonathan M and Moore, Jason H},
	month = oct,
	year = {2012},
	pmid = {23025260},
	pmcid = {PMC3605108},
	keywords = {read 20\%, synthetic datasets},
	pages = {16},
}

@misc{brownlee_how_2018,
	title = {How to {Generate} {Test} {Datasets} in {Python} with scikit-learn},
	url = {https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/},
	abstract = {Test datasets are small contrived datasets that let you test a machine learning algorithm or test harness. The data from test datasets have well-defined properties, such as linearly or non-linearity, that allow you to explore specific algorithm behavior. The scikit-learn Python library provides a suite of functions for generating samples from configurable test problems for […]},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2018},
	keywords = {read 40\%, read 60\%, read 80\%, synthetic datasets},
}

@misc{ahemad_generating_2019,
	title = {Generating {Synthetic} {Classification} {Data} using {Scikit}},
	url = {https://towardsdatascience.com/https-medium-com-faizanahemad-generating-synthetic-classification-data-using-scikit-1590c1632922},
	abstract = {Creating Artificial Data for fast testing of Classifier Performance},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Ahemad, Faizan},
	month = mar,
	year = {2019},
	keywords = {read 40\%, read 60\%, read 80\%, synthetic datasets},
}

@misc{brownlee_tour_2020,
	title = {Tour of {Evaluation} {Metrics} for {Imbalanced} {Classification}},
	url = {https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/},
	abstract = {A classifier is only as good as the metric used to evaluate it. If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model. Choosing an appropriate metric is challenging generally in applied machine […]},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2020},
	keywords = {metrics, read 40\%, read 60\%},
}

@article{bommert_benchmark_2020,
	title = {Benchmark for filter methods for feature selection in high-dimensional classification data},
	volume = {143},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S016794731930194X},
	doi = {10.1016/j.csda.2019.106839},
	abstract = {Feature selection is one of the most fundamental problems in machine learning and has drawn increasing attention due to high-dimensional data sets emerging from different fields like bioinformatics. For feature selection, filter methods play an important role, since they can be combined with any machine learning model and can heavily reduce run time of machine learning algorithms. The aim of the analyses is to review how different filter methods work, to compare their performance with respect to both run time and predictive accuracy, and to provide guidance for applications. Based on 16 high-dimensional classification data sets, 22 filter methods are analyzed with respect to run time and accuracy when combined with a classification method. It is concluded that there is no group of filter methods that always outperforms all other methods, but recommendations on filter methods that perform well on many of the data sets are made. Also, groups of filters that are similar with respect to the order in which they rank the features are found. For the analyses, the R machine learning package mlr is used. It provides a uniform programming API and therefore is a convenient tool to conduct feature selection using filter methods.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Computational Statistics \& Data Analysis},
	author = {Bommert, Andrea and Sun, Xudong and Bischl, Bernd and Rahnenführer, Jörg and Lang, Michel},
	month = mar,
	year = {2020},
	keywords = {benchmark, read 20\%},
	pages = {106839},
}

@phdthesis{kniberg_benchmark_nodate,
	title = {A {Benchmark} of {Prevalent} {Feature} {Selection} {Algorithms} on a {Diverse} {Set} of {Classification} {Problems}},
	abstract = {Feature selection is the process of automatically selecting important features from data. It is an essential part of machine learning, artificial intelligence, data mining, and modelling in general. There are many feature selection algorithms available and the appropriate choice can be difficult. The aim of this thesis was to compare feature selection algorithms in order to provide an experimental basis for which algorithm to choose. The first phase involved assessing which algorithms are most common in the scientific community, through a systematic literature study in the two largest reference databases: Scopus and Web of Science. The second phase involved constructing and implementing a benchmark pipeline to compare 31 algorithms’ performance on 50 data sets. The selected features were used to construct classification models and their predictive performances were compared, as well as the runtime of the selection process. The results show a small overall superiority of embedded type algorithms, especially types that involve Decision Trees. However, there is no algorithm that is significantly superior in every case. The pipeline and data from the experiments can be used by practitioners in determining which algorithms to apply to their respective problems.},
	author = {Kniberg, Anette and Nokto, David},
	keywords = {benchmark},
}

@incollection{guyon_result_2005,
	title = {Result {Analysis} of the {NIPS} 2003 {Feature} {Selection} {Challenge}},
	url = {http://papers.nips.cc/paper/2728-result-analysis-of-the-nips-2003-feature-selection-challenge.pdf},
	urldate = {2020-09-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 17},
	publisher = {MIT Press},
	author = {Guyon, Isabelle and Gunn, Steve and Ben-Hur, Asa and Dror, Gideon},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	year = {2005},
	keywords = {benchmark},
	pages = {545--552},
}

@inproceedings{roffo_infinite_2015,
	address = {Santiago, Chile},
	title = {Infinite {Feature} {Selection}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410835/},
	doi = {10.1109/ICCV.2015.478},
	abstract = {Filter-based feature selection has become crucial in many classiﬁcation settings, especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper, we propose a feature selection method exploiting the convergence properties of power series of matrices, and introducing the concept of inﬁnite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions and letting these paths tend to an inﬁnite number permits the investigation of the importance (relevance and redundancy) of a feature when injected into an arbitrary set of cues. Ranking the importance individuates candidate features, which turn out to be effective from a classiﬁcation point of view, as proved by a thoroughly experimental section. The Inf-FS has been tested on thirteen diverse benchmarks, comparing against ﬁlters, embedded methods, and wrappers; in all the cases we achieve top performances, notably on the classiﬁcation tasks of PASCAL VOC 2007-2012.},
	language = {en},
	urldate = {2020-09-23},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Roffo, Giorgio and Melzi, Simone and Cristani, Marco},
	month = dec,
	year = {2015},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {4202--4210},
}

@misc{sarkar_random_2018,
	title = {Random regression and classification problem generation with symbolic expression},
	url = {https://towardsdatascience.com/random-regression-and-classification-problem-generation-with-symbolic-expression-a4e190e37b8d},
	abstract = {We describe how using SymPy, we can set up random sample generators for polynomial (and nonlinear) regression and classification problems.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Sarkar, Tirthajyoti},
	month = dec,
	year = {2018},
	keywords = {synthetic datasets},
}

@misc{alake_how_2020,
	title = {How {You} {Should} {Read} {Research} {Papers} {According} {To} {Andrew} {Ng} ({Stanford} {Deep} {Learning} {Lectures})},
	url = {https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3},
	abstract = {Instructions on how to approach knowledge acquisition through published research papers by a recognized figure.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Alake, Richmond},
	month = aug,
	year = {2020},
	keywords = {read 100\%, read 40\%, read 60\%, read 80\%, thesis},
}

@article{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	urldate = {2020-09-23},
	journal = {arXiv:1703.03400 [cs]},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {arXiv: 1703.03400},
	keywords = {meta-learning},
}

@article{delgado_why_2019,
	title = {Why {Cohen}’s {Kappa} should be avoided as performance measure in classification},
	volume = {14},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6762152/},
	doi = {10.1371/journal.pone.0222916},
	abstract = {We show that Cohen’s Kappa and Matthews Correlation Coefficient (MCC), both extended and contrasted measures of performance in multi-class classification, are correlated in most situations, albeit can differ in others. Indeed, although in the symmetric case both match, we consider different unbalanced situations in which Kappa exhibits an undesired behaviour, i.e. a worse classifier gets higher Kappa score, differing qualitatively from that of MCC. The debate about the incoherence in the behaviour of Kappa revolves around the convenience, or not, of using a relative metric, which makes the interpretation of its values difficult. We extend these concerns by showing that its pitfalls can go even further. Through experimentation, we present a novel approach to this topic. We carry on a comprehensive study that identifies an scenario in which the contradictory behaviour among MCC and Kappa emerges. Specifically, we find out that when there is a decrease to zero of the entropy of the elements out of the diagonal of the confusion matrix associated to a classifier, the discrepancy between Kappa and MCC rise, pointing to an anomalous performance of the former. We believe that this finding disables Kappa to be used in general as a performance measure to compare classifiers.},
	number = {9},
	urldate = {2020-09-23},
	journal = {PLoS ONE},
	author = {Delgado, Rosario and Tibau, Xavier-Andoni},
	month = sep,
	year = {2019},
	pmid = {31557204},
	pmcid = {PMC6762152},
	keywords = {metrics},
}

@article{atz_tau_nodate,
	title = {The {Tau} of {Data}: {A} {New} {Metric} to {Assess} the {Timeliness} of {Data} in {Catalogues}},
	abstract = {We review existing studies that assess the timeliness of data in catalogues and propose a new metric: tau, the percentage of datasets up-to-date in a data catalogue. Obsolete data will stifle innovation, whereas spotlighting timeliness can foster efficiency and support the sustainability of the open data ecosystem, for example, by encouraging automated publication of data.We validate the tau in three case studies: the World Bank catalogue, the UK data catalogue (data.gov.uk) and the London Datastore. For the World Bank and London we find that roughly half of the datasets are up-to-date, whereas data.gov.uk performs worse.},
	language = {en},
	author = {Atz, Ulrich},
	keywords = {metrics},
	pages = {11},
}
