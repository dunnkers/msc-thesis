
@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/start},
	urldate = {2020-10-21},
}

@inproceedings{parker_analysis_2011,
	title = {An {Analysis} of {Performance} {Measures} for {Binary} {Classifiers}},
	url = {https://ieeexplore.ieee.org/document/6137256},
	doi = {10.1109/ICDM.2011.21},
	abstract = {If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining if one classifier is better than another, given the same test data. Five of these are long established and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate, and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly towards one of the newer methods.},
	booktitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining}},
	author = {Parker, Charles},
	month = dec,
	year = {2011},
	note = {ISSN: 2374-8486},
	keywords = {Evaluation metrics},
	pages = {517--526},
}

@article{alcobaca_mfe_2020,
	title = {{MFE}: {Towards} reproducible meta-feature extraction},
	volume = {21},
	shorttitle = {{MFE}},
	url = {https://jmlr.org/papers/v21/19-348.html},
	abstract = {Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.},
	journal = {Journal of Machine Learning Research},
	author = {Alcobaça, Edesio and Siqueira, Felipe and Garcia, Luís Paulo and Rivolli, Adriano and Oliva, Jefferson and de Carvalho, Andre},
	month = jan,
	year = {2020},
	keywords = {Metafeatures, read 20\%, read 40\%},
	pages = {1--5},
}

@inproceedings{john_irrelevant_1994,
	title = {Irrelevant {Features} and the {Subset} {Selection} {Problem}},
	doi = {10.1016/b978-1-55860-335-6.50023-4},
	abstract = {We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.},
	booktitle = {{ICML}},
	author = {John, George H. and Kohavi, R. and Pfleger, Karl},
	year = {1994},
	keywords = {Feature Selection benchmark, read 20\%},
}

@article{oh_feature_2019,
	title = {Feature {Interaction} in {Terms} of {Prediction} {Performance}},
	volume = {9},
	url = {https://www.researchgate.net/publication/337645592_Feature_Interaction_in_Terms_of_Prediction_Performance},
	doi = {10.3390/app9235191},
	abstract = {There has been considerable development in machine learning in recent years with some remarkable successes. Although there are many high-performance methods, the interpretation of learning models remains challenging. Understanding the underlying theory behind the specific prediction of various models is difficult. Various studies have attempted to explain the working principle behind learning models using techniques like feature importance, partial dependency, feature interaction, and the Shapley value. This study introduces a new feature interaction measure. While recent studies have measured feature interaction using partial dependency, this study redefines feature interaction in terms of prediction performance. The proposed measure is easy to interpret, faster than partial dependency-based measures, and useful to explain feature interaction, which affects prediction performance in both regression and classification models.},
	journal = {Applied Sciences},
	author = {Oh, Sejong},
	month = nov,
	year = {2019},
	keywords = {Feature Interactions, read 20\%},
	pages = {5191},
}

@inproceedings{zhao_searching_2007,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'07},
	title = {Searching for interacting features},
	url = {https://www.semanticscholar.org/paper/Searching-for-Interacting-Features-Zhao-Liu/d2debe138a9b67d838b11d622651383322934aee},
	abstract = {Feature interaction presents a challenge to feature selection for classification. A feature by itself may have little correlation with the target concept, but when it is combined with some other features, they can be strongly correlated with the target concept. Unintentional removal of these features can result in poor classification performance. Handling feature interaction can be computationally intractable. Recognizing the presence of feature interaction, we propose to efficiently handle feature interaction to achieve efficient feature selection and present extensive experimental results of evaluation.},
	urldate = {2020-10-21},
	booktitle = {Proceedings of the 20th international joint conference on {Artifical} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Zhao, Zheng and Liu, Huan},
	month = jan,
	year = {2007},
	keywords = {Feature Interactions, Feature Selection algorithm, read 20\%},
	pages = {1156--1161},
}

@article{guyon_design_nodate,
	title = {Design of experiments for the {NIPS} 2003 variable selection benchmark},
	url = {https://www.semanticscholar.org/paper/Design-of-experiments-for-the-NIPS-2003-variable-Guyon/b979fa88ca448fb08633f961131f45214b1cf109},
	language = {en},
	author = {Guyon, Isabelle},
	keywords = {Dataset generation, Feature Selection benchmark, read 40\%},
	pages = {30},
}

@misc{noauthor_discovering_nodate,
	title = {Discovering {Interaction} {Effects} in {Ensemble} {Models}},
	url = {https://blog.macuyiko.com/post/2019/discovering-interaction-effects-in-ensemble-models.html},
	urldate = {2020-10-12},
	keywords = {Feature Interactions},
}

@misc{noauthor_22_nodate,
	title = {2.2. {Manifold} learning — scikit-learn 0.23.2 documentation},
	url = {https://scikit-learn.org/stable/modules/manifold.html#manifold},
	urldate = {2020-10-12},
	keywords = {Dimensionality reduction, Manifold learning},
}

@misc{derksen_visualising_2019,
	title = {Visualising high-dimensional datasets using {PCA} and t-{SNE} in {Python}},
	url = {https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b},
	abstract = {Update: April 29, 2019. Updated some of the code to not use ggplot but instead use seaborn and matplotlib. I also added an example for a…},
	language = {en},
	urldate = {2020-10-12},
	journal = {Medium},
	author = {Derksen, Luuk},
	month = apr,
	year = {2019},
	keywords = {Dimensionality reduction, Manifold learning},
}

@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	number = {86},
	urldate = {2020-10-12},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	keywords = {Dimensionality reduction, read 20\%},
	pages = {2579--2605},
}

@article{wattenberg_how_2016,
	title = {How to {Use} t-{SNE} {Effectively}},
	volume = {1},
	issn = {2476-0757},
	url = {http://distill.pub/2016/misread-tsne},
	doi = {10.23915/distill.00002},
	abstract = {Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.},
	language = {en},
	number = {10},
	urldate = {2020-10-12},
	journal = {Distill},
	author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
	month = oct,
	year = {2016},
	keywords = {Dimensionality reduction, read 20\%, read 40\%},
	pages = {e2},
}

@article{reis_featsel_2017,
	title = {featsel: {A} framework for benchmarking of feature selection algorithms and cost functions},
	volume = {6},
	issn = {2352-7110},
	shorttitle = {featsel},
	url = {http://www.sciencedirect.com/science/article/pii/S2352711017300286},
	doi = {10.1016/j.softx.2017.07.005},
	abstract = {In this paper, we introduce featsel, a framework for benchmarking of feature selection algorithms and cost functions. This framework allows the user to deal with the search space as a Boolean lattice and has its core coded in C++ for computational efficiency purposes. Moreover, featsel includes Perl scripts to add new algorithms and/or cost functions, generate random instances, plot graphs and organize results into tables. Besides, this framework already comes with dozens of algorithms and cost functions for benchmarking experiments. We also provide illustrative examples, in which featsel outperforms the popular Weka workbench in feature selection procedures on data sets from the UCI Machine Learning Repository.},
	language = {en},
	urldate = {2020-09-23},
	journal = {SoftwareX},
	author = {Reis, Marcelo S. and Estrela, Gustavo and Ferreira, Carlos Eduardo and Barrera, Junior},
	month = jan,
	year = {2017},
	keywords = {Feature Selection benchmark, to read},
	pages = {193--197},
}

@article{parker_measuring_2013,
	title = {On measuring the performance of binary classifiers},
	volume = {35},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-012-0558-x},
	doi = {10.1007/s10115-012-0558-x},
	abstract = {If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining whether one classifier is better than another, given the same test data. Five of these are long established, and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly toward one of the newer methods.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Knowledge and Information Systems},
	author = {Parker, Charles},
	month = apr,
	year = {2013},
	keywords = {Evaluation metrics},
	pages = {131--152},
}

@inproceedings{flach_coherent_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {A coherent interpretation of {AUC} as a measure of aggregated classification performance},
	isbn = {978-1-4503-0619-5},
	abstract = {The area under the ROC curve (AUC), a well-known measure of ranking performance, is also often used as a measure of classification performance, aggregating over decision thresholds as well as class and cost skews. However, David Hand has recently argued that AUC is fundamentally incoherent as a measure of aggregated classifier performance and proposed an alternative measure (Hand, 2009). Specifically, Hand derives a linear relationship between AUC and expected minimum loss, where the expectation is taken over a distribution of the misclassification cost parameter that depends on the model under consideration. Replacing this distribution with a Beta(2,2) distribution, Hand derives his alternative measure H. In this paper we offer an alternative, coherent interpretation of AUC as linearly related to expected loss. We use a distribution over cost parameter and a distribution over data points, both uniform and hence model-independent. Should one wish to consider only optimal thresholds, we demonstrate that a simple and more intuitive alternative to Hand's H measure is already available in the form of the area under the cost curve.},
	urldate = {2020-10-09},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Flach, Peter and Hernández-Orallo, José and Ferri, Cèsar},
	month = jun,
	year = {2011},
	keywords = {Evaluation metrics},
	pages = {657--664},
}

@article{hand_measuring_2009,
	title = {Measuring classifier performance: a coherent alternative to the area under the {ROC} curve},
	volume = {77},
	issn = {1573-0565},
	shorttitle = {Measuring classifier performance},
	url = {https://doi.org/10.1007/s10994-009-5119-5},
	doi = {10.1007/s10994-009-5119-5},
	abstract = {The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Machine Learning},
	author = {Hand, David J.},
	month = oct,
	year = {2009},
	keywords = {Evaluation metrics},
	pages = {103--123},
}

@misc{ahemad_selecting_2019,
	title = {Selecting the {Right} {Metric} for {Skewed} {Classification} {Problems}},
	url = {https://towardsdatascience.com/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7},
	abstract = {Skew Accuracy!! Lets try few other classification metrics!!},
	language = {en},
	urldate = {2020-10-09},
	journal = {Medium},
	author = {Ahemad, Faizan},
	month = mar,
	year = {2019},
	keywords = {Evaluation metrics, read 20\%, read 40\%},
}

@inproceedings{katz_explorekit_2016,
	address = {Barcelona, Spain},
	title = {{ExploreKit}: {Automatic} {Feature} {Generation} and {Selection}},
	isbn = {9781509054732},
	shorttitle = {{ExploreKit}},
	url = {http://ieeexplore.ieee.org/document/7837936/},
	doi = {10.1109/ICDM.2016.0123},
	urldate = {2020-10-08},
	booktitle = {2016 {IEEE} 16th {International} {Conference} on {Data} {Mining} ({ICDM})},
	publisher = {IEEE},
	author = {Katz, Gilad and Shin, Eui Chul Richard and Song, Dawn},
	month = dec,
	year = {2016},
	keywords = {read 20\%},
	pages = {979--984},
}

@inproceedings{le_using_2017,
	title = {Using synthetic data to train neural networks is model-based reasoning},
	url = {https://arxiv.org/pdf/1703.00868.pdf},
	doi = {10.1109/IJCNN.2017.7966298},
	abstract = {We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important considerations for ensuring good neural network generalization when training with synthetic data.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Le, Tuan Anh and Baydin, Atilim Giineş and Zinkov, Robert and Wood, Frank},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Dataset generation, read 20\%},
	pages = {3514--3521},
}

@article{tripathi_learning_2019,
	title = {Learning to {Generate} {Synthetic} {Data} via {Compositing}},
	url = {http://arxiv.org/abs/1904.05475},
	abstract = {We present a task-aware approach to synthetic data generation. Our framework employs a trainable synthesizer network that is optimized to produce meaningful training samples by assessing the strengths and weaknesses of a `target' network. The synthesizer and target networks are trained in an adversarial manner wherein each network is updated with a goal to outdo the other. Additionally, we ensure the synthesizer generates realistic data by pairing it with a discriminator trained on real-world images. Further, to make the target classifier invariant to blending artefacts, we introduce these artefacts to background regions of the training images so the target does not over-fit to them. We demonstrate the efficacy of our approach by applying it to different target networks including a classification network on AffNIST, and two object detection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST benchmark, our approach is able to surpass the baseline results with just half the training examples. On the VOC person detection benchmark, we show improvements of up to 2.7\% as a result of our data augmentation. Similarly on the GMU detection benchmark, we report a performance boost of 3.5\% in mAP over the baseline method, outperforming the previous state of the art approaches by up to 7.5\% on specific categories.},
	urldate = {2020-10-08},
	journal = {arXiv:1904.05475 [cs]},
	author = {Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and Tyagi, Ambrish and Rehg, James M. and Chari, Visesh},
	month = jul,
	year = {2019},
	note = {arXiv: 1904.05475},
	keywords = {Dataset generation, read 20\%},
}

@article{rivolli_characterizing_2019,
	title = {Characterizing classification datasets: a study of meta-features for meta-learning},
	shorttitle = {Characterizing classification datasets},
	url = {http://arxiv.org/abs/1808.10406},
	abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
	urldate = {2020-10-08},
	journal = {arXiv:1808.10406 [cs, stat]},
	author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
	month = aug,
	year = {2019},
	note = {arXiv: 1808.10406
version: 2},
	keywords = {Meta-Learning, Metafeatures, read 20\%},
}

@article{vanschoren_meta-learning_2018,
	title = {Meta-{Learning}: {A} {Survey}},
	shorttitle = {Meta-{Learning}},
	url = {http://arxiv.org/abs/1810.03548},
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
	urldate = {2020-09-29},
	journal = {arXiv:1810.03548 [cs, stat]},
	author = {Vanschoren, Joaquin},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03548},
	keywords = {Meta-Learning, Metafeatures, read 20\%, read 40\%, read 60\%},
}

@article{pinto_autobagging_2017,
	title = {{autoBagging}: {Learning} to {Rank} {Bagging} {Workflows} with {Metalearning}},
	shorttitle = {{autoBagging}},
	url = {http://arxiv.org/abs/1706.09367},
	abstract = {Machine Learning (ML) has been successfully applied to a wide range of domains and applications. One of the techniques behind most of these successful applications is Ensemble Learning (EL), the field of ML that gave birth to methods such as Random Forests or Boosting. The complexity of applying these techniques together with the market scarcity on ML experts, has created the need for systems that enable a fast and easy drop-in replacement for ML libraries. Automated machine learning (autoML) is the field of ML that attempts to answers these needs. Typically, these systems rely on optimization techniques such as bayesian optimization to lead the search for the best model. Our approach differs from these systems by making use of the most recent advances on metalearning and a learning to rank approach to learn from metadata. We propose autoBagging, an autoML system that automatically ranks 63 bagging workflows by exploiting past performance and dataset characterization. Results on 140 classification datasets from the OpenML platform show that autoBagging can yield better performance than the Average Rank method and achieve results that are not statistically different from an ideal model that systematically selects the best workflow for each dataset. For the purpose of reproducibility and generalizability, autoBagging is publicly available as an R package on CRAN.},
	urldate = {2020-10-06},
	journal = {arXiv:1706.09367 [cs, stat]},
	author = {Pinto, Fábio and Cerqueira, Vítor and Soares, Carlos and Mendes-Moreira, João},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09367},
	keywords = {Metafeatures, read 20\%},
}

@article{arik_tabnet_2020,
	title = {{TabNet}: {Attentive} {Interpretable} {Tabular} {Learning}},
	shorttitle = {{TabNet}},
	url = {http://arxiv.org/abs/1908.07442},
	abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
	urldate = {2020-09-29},
	journal = {arXiv:1908.07442 [cs, stat]},
	author = {Arik, Sercan O. and Pfister, Tomas},
	month = feb,
	year = {2020},
	note = {arXiv: 1908.07442},
	keywords = {Feature Importance, to read},
}

@incollection{vanschoren_meta-learning_2019,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Meta-{Learning}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_2},
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Vanschoren, Joaquin},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_2},
	keywords = {Meta-Learning, read 20\%},
	pages = {35--61},
}

@article{triantafillou_meta-dataset_2020,
	title = {Meta-{Dataset}: {A} {Dataset} of {Datasets} for {Learning} to {Learn} from {Few} {Examples}},
	shorttitle = {Meta-{Dataset}},
	url = {http://arxiv.org/abs/1903.03096},
	abstract = {Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.},
	urldate = {2020-09-29},
	journal = {arXiv:1903.03096 [cs, stat]},
	author = {Triantafillou, Eleni and Zhu, Tyler and Dumoulin, Vincent and Lamblin, Pascal and Evci, Utku and Xu, Kelvin and Goroshin, Ross and Gelada, Carles and Swersky, Kevin and Manzagol, Pierre-Antoine and Larochelle, Hugo},
	month = apr,
	year = {2020},
	note = {arXiv: 1903.03096},
	keywords = {Meta-Learning, read 20\%},
}

@article{greenwell_simple_2018,
	title = {A {Simple} and {Effective} {Model}-{Based} {Variable} {Importance} {Measure}},
	url = {http://arxiv.org/abs/1805.04755},
	abstract = {In the era of "big data", it is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. For example, it is often of interest to know which, if any, of the predictors in a fitted model are relatively influential on the predicted outcome. Some modern algorithms---like random forests and gradient boosted decision trees---have a natural way of quantifying the importance or relative influence of each feature. Other algorithms---like naive Bayes classifiers and support vector machines---are not capable of doing so and model-free approaches are generally used to measure each predictor's importance. In this paper, we propose a standardized, model-based approach to measuring predictor importance across the growing spectrum of supervised learning algorithms. Our proposed method is illustrated through both simulated and real data examples. The R code to reproduce all of the figures in this paper is available in the supplementary materials.},
	urldate = {2020-09-25},
	journal = {arXiv:1805.04755 [cs, stat]},
	author = {Greenwell, Brandon M. and Boehmke, Bradley C. and McCarthy, Andrew J.},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04755},
	keywords = {Feature Importance ranking, Feature Interactions, read 20\%},
}

@article{dash_feature_1997,
	title = {Feature {Selection} for {Classification}},
	doi = {10.1016/S1088-467X(97)00008-5},
	abstract = {Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.},
	journal = {Intell. Data Anal.},
	author = {Dash, M. and Liu, Huan},
	year = {1997},
	keywords = {Feature Selection algorithm review, Feature Selection benchmark, read 20\%, read 40\%},
}

@inproceedings{jovic_review_2015,
	title = {A review of feature selection methods with applications},
	doi = {10.1109/MIPRO.2015.7160458},
	abstract = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
	booktitle = {2015 38th {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	author = {Jović, A. and Brkić, K. and Bogunović, N.},
	month = may,
	year = {2015},
	keywords = {Feature Selection algorithm review, read 20\%, read 40\%},
	pages = {1200--1205},
}

@article{bolon-canedo_review_2013,
	title = {A review of feature selection methods on synthetic data},
	volume = {34},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-012-0487-8},
	doi = {10.1007/s10115-012-0487-8},
	abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
	language = {en},
	number = {3},
	urldate = {2020-09-25},
	journal = {Knowledge and Information Systems},
	author = {Bolón-Canedo, Verónica and Sánchez-Maroño, Noelia and Alonso-Betanzos, Amparo},
	month = mar,
	year = {2013},
	keywords = {Dataset generation, Feature Selection benchmark, read 20\%, read 40\%, read 60\%},
	pages = {483--519},
}

@misc{klein_machine_nodate,
	title = {Machine {Learning} with {Python}: {Create} {Artificial} {Datasets} for {Machine} {Learning} {Usage} with {Scikit}-{Learn}},
	url = {https://www.python-course.eu/machine_learning_create_datasets.php},
	urldate = {2020-09-23},
	author = {Klein, Bernd},
	keywords = {Dataset generation},
}

@misc{vu_scikit-learn_nodate,
	title = {Scikit-{Learn} \& {More} for {Synthetic} {Dataset} {Generation} for {Machine} {Learning}},
	url = {https://www.kdnuggets.com/scikit-learn-more-for-synthetic-dataset-generation-for-machine-learning.html/},
	abstract = {While mature algorithms and extensive open-source libraries are widely available for machine learning practitioners, sufficient data to apply these techniques remains a core challenge. Discover how to leverage scikit-learn and other tools to generate synthetic data appropriate for optimizing and fine-tuning your models.},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {KDnuggets},
	author = {Vu, Kevin},
	keywords = {Dataset generation, read 100\%, read 40\%, read 60\%, read 80\%},
}

@misc{weng_meta-learning_2018,
	title = {Meta-{Learning}: {Learning} to {Learn} {Fast}},
	shorttitle = {Meta-{Learning}},
	url = {https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html},
	abstract = {Meta-learning, also known as “learning to learn”, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples. There are three common approaches: 1) learn an efficient distance metric (metric-based); 2) use (recurrent) network with external or internal memory (model-based); 3)...},
	language = {en},
	urldate = {2020-09-23},
	journal = {Lil'Log},
	author = {Weng, Lilian},
	month = nov,
	year = {2018},
	keywords = {Meta-Learning},
}

@article{friedman_predictive_2008,
	title = {Predictive learning via rule ensembles},
	volume = {2},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0811.1679},
	doi = {10.1214/07-AOAS148},
	abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
	number = {3},
	urldate = {2020-09-25},
	journal = {The Annals of Applied Statistics},
	author = {Friedman, Jerome H. and Popescu, Bogdan E.},
	month = sep,
	year = {2008},
	note = {arXiv: 0811.1679},
	keywords = {Feature Importance ranking, Feature Interactions, read 20\%},
	pages = {916--954},
}

@inproceedings{okimoto_complexity_2017,
	title = {Complexity {Measures} {Effectiveness} in {Feature} {Selection}},
	doi = {10.1109/BRACIS.2017.66},
	abstract = {Feature selection is an important pre-processing step usually mandatory in data analysis by Machine Learning techniques. Its objective is to reduce data dimensionality by removing irrelevant and redundant features from a dataset. In this work we investigate how the presence of irrelevant features in a dataset affects the complexity of a classification problem solution. This is performed by monitoring the values of some complexity measures extracted from the original and preprocessed datasets. These descriptors allow estimating the intrinsic difficulty of a classification problem. Some of these measures are then used in feature ranking. The results are promising and reveal that the complexity measures are indeed suitable for estimating feature importance in classification datasets.},
	booktitle = {2017 {Brazilian} {Conference} on {Intelligent} {Systems} ({BRACIS})},
	author = {Okimoto, Lucas Chesini and Savii, Ricardo Manhães and Lorena, Ana Carolina},
	month = oct,
	year = {2017},
	keywords = {Dataset properties, read 20\%, to read},
	pages = {91--96},
}

@inproceedings{patki_synthetic_2016,
	title = {The {Synthetic} {Data} {Vault}},
	doi = {10.1109/DSAA.2016.49},
	abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	month = oct,
	year = {2016},
	keywords = {Dataset generation, read 20\%, to read},
	pages = {399--410},
}

@inproceedings{luxburg_clustering_2012,
	title = {Clustering: {Science} or {Art}?},
	shorttitle = {Clustering},
	url = {http://proceedings.mlr.press/v27/luxburg12a.html},
	abstract = {We examine whether the quality of different clustering algorithms can be compared by a general, scientifically sound procedure which is independent of particular clustering algorithms. We argue tha...},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Proceedings of {ICML} {Workshop} on {Unsupervised} and {Transfer} {Learning}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Luxburg, Ulrike von and Williamson, Robert C. and Guyon, Isabelle},
	month = jun,
	year = {2012},
	keywords = {Dataset generation, read 20\%},
	pages = {65--79},
}

@article{franti_k-means_2018,
	title = {K-means properties on six clustering benchmark datasets},
	volume = {48},
	issn = {0924-669X},
	url = {https://link.springer.com/epdf/10.1007/s10489-018-1238-7},
	doi = {10.1007/s10489-018-1238-7},
	abstract = {This paper has two contributions. First, we introduce a clustering basic benchmark. Second, we study the performance of k-means using this benchmark. Specifically, we measure how the performance depends on four factors: (1) overlap of clusters, (2) number of clusters, (3) dimensionality, and (4) unbalance of cluster sizes. The results show that overlap is critical, and that k-means starts to work effectively when the overlap reaches 4\% level.},
	language = {en},
	number = {12},
	urldate = {2020-09-24},
	journal = {Applied Intelligence},
	author = {Fränti, Pasi and Sieranoja, Sami},
	year = {2018},
	keywords = {Dataset generation, read 20\%, read 40\%},
}

@inproceedings{bouneffouf_sampling_2015,
	title = {Sampling with {Minimum} {Sum} of {Squared} {Similarities} for {Nystrom}-{Based} {Large} {Scale} {Spectral} {Clustering}},
	abstract = {The Nystrom sampling provides an efficient approach for large scale clustering problems, by generating a low-rank matrix approximation. However, existing sampling methods are limited by their accuracies and computing times. This paper proposes a scalable Nystrom-based clustering algorithm with a new sampling procedure, Minimum Sum of Squared Similarities (MSSS). Here we provide a theoretical analysis of the upper error bound of our algorithm, and demonstrate its performance in comparison to the leading spectral clustering methods that use Nystrom sampling.},
	booktitle = {{IJCAI}},
	author = {Bouneffouf, Djallel and Birol, I.},
	year = {2015},
	keywords = {Dataset generation, read 20\%},
}

@misc{sarkar_synthetic_2019,
	title = {Synthetic data generation — a must-have skill for new data scientists},
	url = {https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae},
	abstract = {A brief rundown of packages and ideas to generate synthetic data for self-driven data science projects and deep diving into machine…},
	language = {en},
	urldate = {2020-09-24},
	journal = {Medium},
	author = {Sarkar, Tirthajyoti},
	month = jul,
	year = {2019},
	keywords = {Dataset generation, read 20\%, read 40\%, read 60\%},
}

@article{zeng_novel_2015,
	title = {A novel feature selection method considering feature interaction},
	volume = {48},
	issn = {0031-3203},
	url = {https://doi.org/10.1016/j.patcog.2015.02.025},
	doi = {10.1016/j.patcog.2015.02.025},
	abstract = {Interacting features are those that appear to be irrelevant or weakly relevant with the class individually, but when it combined with other features, it may highly correlate to the class. Discovering feature interaction is a challenging task in feature selection. In this paper, a novel feature selection algorithm considering feature interaction is proposed. Firstly, feature relevance, feature redundancy and feature interaction have been redefined in the framework of information theory. Then the interaction weight factor which can reflect the information of whether a feature is redundant or interactive is proposed. Afterwards, we bring forward an Interaction Weight based Feature Selection algorithm (IWFS). To evaluate the performance of the proposed algorithm, we compare IWFS with other five representative feature selection algorithms, including CFS, INTERACT, FCBF, MRMR and Relief-F, in terms of the classification accuracies and the number of selected features with three different types of classifiers including C4.5, IB1 and PART. The results on the six synthetic datasets show that IWFS can effectively identify irrelevant and redundant features while reserving interactive ones. The results on the eight real world datasets indicate that IWFS not only efficiently reduces the dimensionality of feature space, but also offers the highest average accuracy for all the three classification algorithms. A novel feature selection method based on interaction weight factor is proposed.We redefined relevance, redundancy and interaction of features in the framework of information theory.The algorithm can deal with irrelevant, redundant and interactive features.Our method obtains the best average accuracies compared with the other five algorithms.},
	number = {8},
	urldate = {2020-09-23},
	journal = {Pattern Recognition},
	author = {Zeng, Zilin and Zhang, Hongjun and Zhang, Rui and Yin, Chengxiang},
	month = aug,
	year = {2015},
	keywords = {Feature Interactions, Feature Selection algorithm, read 20\%},
	pages = {2656--2666},
}

@article{zhao_searching_2009,
	title = {Searching for interacting features in subset selection},
	volume = {13},
	issn = {1088-467X},
	url = {https://content.iospress.com/articles/intelligent-data-analysis/ida00364},
	doi = {10.3233/IDA-2009-0364},
	abstract = {The evolving and adapting capabilities of robust intelligence are best manifested in its ability to learn. Machine learning enables computer systems to learn, and improve performance. Feature selection facilitates machine learning (e.g., classificati},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Intelligent Data Analysis},
	author = {Zhao, Zheng and Liu, Huan},
	month = jan,
	year = {2009},
	keywords = {Feature Interactions, Feature Selection algorithm, read 20\%},
	pages = {207--228},
}

@inproceedings{kira_feature_1992,
	title = {The {Feature} {Selection} {Problem}: {Traditional} {Methods} and a {New} {Algorithm}},
	shorttitle = {The {Feature} {Selection} {Problem}},
	abstract = {For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Rellef which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as nonoptimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems.},
	booktitle = {{AAAI}},
	author = {Kira, K. and Rendell, L.},
	year = {1992},
	keywords = {Feature Interactions, Feature Selection algorithm, read 20\%},
}

@article{bennasar_feature_2015,
	title = {Feature selection using {Joint} {Mutual} {Information} {Maximisation}},
	volume = {42},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417415004674},
	doi = {10.1016/j.eswa.2015.07.007},
	abstract = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the ‘maximum of the minimum’ criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6\% in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
	language = {en},
	number = {22},
	urldate = {2020-09-23},
	journal = {Expert Systems with Applications},
	author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
	month = dec,
	year = {2015},
	keywords = {Feature Selection algorithm, read 20\%},
	pages = {8520--8532},
}

@article{goyal_feature_2020,
	title = {Feature {Interactions} in {XGBoost}},
	url = {http://arxiv.org/abs/2007.05758},
	abstract = {In this paper, we investigate how feature interactions can be identified to be used as constraints in the gradient boosting tree models using XGBoost's implementation. Our results show that accurate identification of these constraints can help improve the performance of baseline XGBoost model significantly. Further, the improvement in the model structure can also lead to better interpretability.},
	urldate = {2020-09-23},
	journal = {arXiv:2007.05758 [cs, stat]},
	author = {Goyal, Kshitij and Dumancic, Sebastijan and Blockeel, Hendrik},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.05758},
	keywords = {Dataset generation, Feature Interactions, read 20\%, read 40\%},
}

@book{molnar_51_nodate,
	title = {5.1 {Partial} {Dependence} {Plot} ({PDP}) {\textbar} {Interpretable} {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/pdp.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2020-09-23},
	author = {Molnar, Christoph},
	keywords = {Dataset generation, read 20\%},
}

@article{urbanowicz_gametes_2012,
	title = {{GAMETES}: a fast, direct algorithm for generating pure, strict, epistatic models with random architectures},
	volume = {5},
	issn = {1756-0381},
	shorttitle = {{GAMETES}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3605108/},
	doi = {10.1186/1756-0381-5-16},
	abstract = {Background
Geneticists who look beyond single locus disease associations require additional strategies for the detection of complex multi-locus effects. Epistasis, a multi-locus masking effect, presents a particular challenge, and has been the target of bioinformatic development. Thorough evaluation of new algorithms calls for simulation studies in which known disease models are sought. To date, the best methods for generating simulated multi-locus epistatic models rely on genetic algorithms. However, such methods are computationally expensive, difficult to adapt to multiple objectives, and unlikely to yield models with a precise form of epistasis which we refer to as pure and strict. Purely and strictly epistatic models constitute the worst-case in terms of detecting disease associations, since such associations may only be observed if all n-loci are included in the disease model. This makes them an attractive gold standard for simulation studies considering complex multi-locus effects.

Results
We introduce GAMETES, a user-friendly software package and algorithm which generates complex biallelic single nucleotide polymorphism (SNP) disease models for simulation studies. GAMETES rapidly and precisely generates random, pure, strict n-locus models with specified genetic constraints. These constraints include heritability, minor allele frequencies of the SNPs, and population prevalence. GAMETES also includes a simple dataset simulation strategy which may be utilized to rapidly generate an archive of simulated datasets for given genetic models. We highlight the utility and limitations of GAMETES with an example simulation study using MDR, an algorithm designed to detect epistasis.

Conclusions
GAMETES is a fast, flexible, and precise tool for generating complex n-locus models with random architectures. While GAMETES has a limited ability to generate models with higher heritabilities, it is proficient at generating the lower heritability models typically used in simulation studies evaluating new algorithms. In addition, the GAMETES modeling strategy may be flexibly combined with any dataset simulation strategy. Beyond dataset simulation, GAMETES could be employed to pursue theoretical characterization of genetic models and epistasis.},
	urldate = {2020-09-23},
	journal = {BioData Mining},
	author = {Urbanowicz, Ryan J and Kiralis, Jeff and Sinnott-Armstrong, Nicholas A and Heberling, Tamra and Fisher, Jonathan M and Moore, Jason H},
	month = oct,
	year = {2012},
	pmid = {23025260},
	pmcid = {PMC3605108},
	keywords = {Dataset generation, read 20\%},
	pages = {16},
}

@article{urbanowicz_benchmarking_2018,
	title = {Benchmarking relief-based feature selection methods for bioinformatics data mining},
	volume = {85},
	issn = {1532-0464},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046418301412},
	doi = {10.1016/j.jbi.2018.07.015},
	abstract = {Modern biomedical data mining requires feature selection methods that can (1) be applied to large scale feature spaces (e.g. ‘omics’ data), (2) function in noisy problems, (3) detect complex patterns of association (e.g. gene-gene interactions), (4) be flexibly adapted to various problem domains and data types (e.g. genetic variants, gene expression, and clinical data) and (5) are computationally tractable. To that end, this work examines a set of filter-style feature selection algorithms inspired by the ‘Relief’ algorithm, i.e. Relief-Based algorithms (RBAs). We implement and expand these RBAs in an open source framework called ReBATE (Relief-Based Algorithm Training Environment). We apply a comprehensive genetic simulation study comparing existing RBAs, a proposed RBA called MultiSURF, and other established feature selection methods, over a variety of problems. The results of this study (1) support the assertion that RBAs are particularly flexible, efficient, and powerful feature selection methods that differentiate relevant features having univariate, multivariate, epistatic, or heterogeneous associations, (2) confirm the efficacy of expansions for classification vs. regression, discrete vs. continuous features, missing data, multiple classes, or class imbalance, (3) identify previously unknown limitations of specific RBAs, and (4) suggest that while MultiSURF∗ performs best for explicitly identifying pure 2-way interactions, MultiSURF yields the most reliable feature selection performance across a wide range of problem types.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Journal of Biomedical Informatics},
	author = {Urbanowicz, Ryan J. and Olson, Randal S. and Schmitt, Peter and Meeker, Melissa and Moore, Jason H.},
	month = sep,
	year = {2018},
	keywords = {Feature Selection benchmark, read 40\%, read 60\%},
	pages = {168--188},
}

@article{urbanowicz_relief-based_2018,
	title = {Relief-based feature selection: {Introduction} and review},
	volume = {85},
	issn = {1532-0464},
	shorttitle = {Relief-based feature selection},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046418301400},
	doi = {10.1016/j.jbi.2018.07.014},
	abstract = {Feature selection plays a critical role in biomedical data mining, driven by increasing feature dimensionality in target problems and growing interest in advanced but computationally expensive methodologies able to model complex associations. Specifically, there is a need for feature selection methods that are computationally efficient, yet sensitive to complex patterns of association, e.g. interactions, so that informative features are not mistakenly eliminated prior to downstream modeling. This paper focuses on Relief-based algorithms (RBAs), a unique family of filter-style feature selection algorithms that have gained appeal by striking an effective balance between these objectives while flexibly adapting to various data characteristics, e.g. classification vs. regression. First, this work broadly examines types of feature selection and defines RBAs within that context. Next, we introduce the original Relief algorithm and associated concepts, emphasizing the intuition behind how it works, how feature weights generated by the algorithm can be interpreted, and why it is sensitive to feature interactions without evaluating combinations of features. Lastly, we include an expansive review of RBA methodological research beyond Relief and its popular descendant, ReliefF. In particular, we characterize branches of RBA research, and provide comparative summaries of RBA algorithms including contributions, strategies, functionality, time complexity, adaptation to key data characteristics, and software availability.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Journal of Biomedical Informatics},
	author = {Urbanowicz, Ryan J. and Meeker, Melissa and La Cava, William and Olson, Randal S. and Moore, Jason H.},
	month = sep,
	year = {2018},
	keywords = {Feature Selection algorithm review, read 20\%},
	pages = {189--203},
}

@book{molnar_54_nodate,
	title = {5.4 {Feature} {Interaction} {\textbar} {Interpretable} {Machine} {Learning}},
	url = {https://christophm.github.io/interpretable-ml-book/interaction.html},
	abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
	urldate = {2020-09-23},
	author = {Molnar, Christoph},
	keywords = {Dataset generation, Dataset properties, Feature Interactions, read 20\%, read 40\%, to read},
}

@misc{brownlee_how_2018,
	title = {How to {Generate} {Test} {Datasets} in {Python} with scikit-learn},
	url = {https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/},
	abstract = {Test datasets are small contrived datasets that let you test a machine learning algorithm or test harness. The data from test datasets have well-defined properties, such as linearly or non-linearity, that allow you to explore specific algorithm behavior. The scikit-learn Python library provides a suite of functions for generating samples from configurable test problems for […]},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2018},
	keywords = {Dataset generation, read 40\%, read 60\%, read 80\%},
}

@misc{ahemad_generating_2019,
	title = {Generating {Synthetic} {Classification} {Data} using {Scikit}},
	url = {https://towardsdatascience.com/https-medium-com-faizanahemad-generating-synthetic-classification-data-using-scikit-1590c1632922},
	abstract = {Creating Artificial Data for fast testing of Classifier Performance},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Ahemad, Faizan},
	month = mar,
	year = {2019},
	keywords = {Dataset generation, read 40\%, read 60\%, read 80\%},
}

@misc{brownlee_tour_2020,
	title = {Tour of {Evaluation} {Metrics} for {Imbalanced} {Classification}},
	url = {https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/},
	abstract = {A classifier is only as good as the metric used to evaluate it. If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model. Choosing an appropriate metric is challenging generally in applied machine […]},
	language = {en-US},
	urldate = {2020-09-23},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2020},
	keywords = {Evaluation metrics, read 40\%, read 60\%},
}

@article{bommert_benchmark_2020,
	title = {Benchmark for filter methods for feature selection in high-dimensional classification data},
	volume = {143},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S016794731930194X},
	doi = {10.1016/j.csda.2019.106839},
	abstract = {Feature selection is one of the most fundamental problems in machine learning and has drawn increasing attention due to high-dimensional data sets emerging from different fields like bioinformatics. For feature selection, filter methods play an important role, since they can be combined with any machine learning model and can heavily reduce run time of machine learning algorithms. The aim of the analyses is to review how different filter methods work, to compare their performance with respect to both run time and predictive accuracy, and to provide guidance for applications. Based on 16 high-dimensional classification data sets, 22 filter methods are analyzed with respect to run time and accuracy when combined with a classification method. It is concluded that there is no group of filter methods that always outperforms all other methods, but recommendations on filter methods that perform well on many of the data sets are made. Also, groups of filters that are similar with respect to the order in which they rank the features are found. For the analyses, the R machine learning package mlr is used. It provides a uniform programming API and therefore is a convenient tool to conduct feature selection using filter methods.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Computational Statistics \& Data Analysis},
	author = {Bommert, Andrea and Sun, Xudong and Bischl, Bernd and Rahnenführer, Jörg and Lang, Michel},
	month = mar,
	year = {2020},
	keywords = {Feature Selection benchmark, read 20\%},
	pages = {106839},
}

@phdthesis{kniberg_benchmark_nodate,
	title = {A {Benchmark} of {Prevalent} {Feature} {Selection} {Algorithms} on a {Diverse} {Set} of {Classification} {Problems}},
	abstract = {Feature selection is the process of automatically selecting important features from data. It is an essential part of machine learning, artificial intelligence, data mining, and modelling in general. There are many feature selection algorithms available and the appropriate choice can be difficult. The aim of this thesis was to compare feature selection algorithms in order to provide an experimental basis for which algorithm to choose. The first phase involved assessing which algorithms are most common in the scientific community, through a systematic literature study in the two largest reference databases: Scopus and Web of Science. The second phase involved constructing and implementing a benchmark pipeline to compare 31 algorithms’ performance on 50 data sets. The selected features were used to construct classification models and their predictive performances were compared, as well as the runtime of the selection process. The results show a small overall superiority of embedded type algorithms, especially types that involve Decision Trees. However, there is no algorithm that is significantly superior in every case. The pipeline and data from the experiments can be used by practitioners in determining which algorithms to apply to their respective problems.},
	author = {Kniberg, Anette and Nokto, David},
	keywords = {Feature Selection benchmark},
}

@incollection{guyon_result_2005,
	title = {Result {Analysis} of the {NIPS} 2003 {Feature} {Selection} {Challenge}},
	url = {http://papers.nips.cc/paper/2728-result-analysis-of-the-nips-2003-feature-selection-challenge.pdf},
	urldate = {2020-09-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 17},
	publisher = {MIT Press},
	author = {Guyon, Isabelle and Gunn, Steve and Ben-Hur, Asa and Dror, Gideon},
	editor = {Saul, L. K. and Weiss, Y. and Bottou, L.},
	year = {2005},
	keywords = {Feature Selection benchmark},
	pages = {545--552},
}

@article{wang_feature_2007,
	title = {Feature selection based on rough sets and particle swarm optimization},
	volume = {28},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865506002327},
	doi = {10.1016/j.patrec.2006.09.003},
	abstract = {We propose a new feature selection strategy based on rough sets and particle swarm optimization (PSO). Rough sets have been used as a feature selection method with much success, but current hill-climbing rough set approaches to feature selection are inadequate at finding optimal reductions as no perfect heuristic can guarantee optimality. On the other hand, complete searches are not feasible for even medium-sized datasets. So, stochastic approaches provide a promising feature selection mechanism. Like Genetic Algorithms, PSO is a new evolutionary computation technique, in which each potential solution is seen as a particle with a certain velocity flying through the problem space. The Particle Swarms find optimal regions of the complex search space through the interaction of individuals in the population. PSO is attractive for feature selection in that particle swarms will discover best feature combinations as they fly within the subset space. Compared with GAs, PSO does not need complex operators such as crossover and mutation, it requires only primitive and simple mathematical operators, and is computationally inexpensive in terms of both memory and runtime. Experimentation is carried out, using UCI data, which compares the proposed algorithm with a GA-based approach and other deterministic rough set reduction algorithms. The results show that PSO is efficient for rough set-based feature selection.},
	language = {en},
	number = {4},
	urldate = {2020-09-23},
	journal = {Pattern Recognition Letters},
	author = {Wang, Xiangyang and Yang, Jie and Teng, Xiaolong and Xia, Weijun and Jensen, Richard},
	month = mar,
	year = {2007},
	keywords = {Feature Selection algorithm},
	pages = {459--471},
}

@inproceedings{roffo_infinite_2015,
	address = {Santiago, Chile},
	title = {Infinite {Feature} {Selection}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410835/},
	doi = {10.1109/ICCV.2015.478},
	abstract = {Filter-based feature selection has become crucial in many classiﬁcation settings, especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper, we propose a feature selection method exploiting the convergence properties of power series of matrices, and introducing the concept of inﬁnite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions and letting these paths tend to an inﬁnite number permits the investigation of the importance (relevance and redundancy) of a feature when injected into an arbitrary set of cues. Ranking the importance individuates candidate features, which turn out to be effective from a classiﬁcation point of view, as proved by a thoroughly experimental section. The Inf-FS has been tested on thirteen diverse benchmarks, comparing against ﬁlters, embedded methods, and wrappers; in all the cases we achieve top performances, notably on the classiﬁcation tasks of PASCAL VOC 2007-2012.},
	language = {en},
	urldate = {2020-09-23},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Roffo, Giorgio and Melzi, Simone and Cristani, Marco},
	month = dec,
	year = {2015},
	keywords = {Feature Selection algorithm},
	pages = {4202--4210},
}

@article{chapman_dataset_2020,
	title = {Dataset search: a survey},
	volume = {29},
	issn = {0949-877X},
	shorttitle = {Dataset search},
	url = {https://doi.org/10.1007/s00778-019-00564-x},
	doi = {10.1007/s00778-019-00564-x},
	abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
	language = {en},
	number = {1},
	urldate = {2020-09-23},
	journal = {The VLDB Journal},
	author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ibáñez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
	month = jan,
	year = {2020},
	keywords = {Datasets},
	pages = {251--272},
}

@misc{sarkar_random_2018,
	title = {Random regression and classification problem generation with symbolic expression},
	url = {https://towardsdatascience.com/random-regression-and-classification-problem-generation-with-symbolic-expression-a4e190e37b8d},
	abstract = {We describe how using SymPy, we can set up random sample generators for polynomial (and nonlinear) regression and classification problems.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Sarkar, Tirthajyoti},
	month = dec,
	year = {2018},
	keywords = {Dataset generation},
}

@misc{alake_how_2020,
	title = {How {You} {Should} {Read} {Research} {Papers} {According} {To} {Andrew} {Ng} ({Stanford} {Deep} {Learning} {Lectures})},
	url = {https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3},
	abstract = {Instructions on how to approach knowledge acquisition through published research papers by a recognized figure.},
	language = {en},
	urldate = {2020-09-23},
	journal = {Medium},
	author = {Alake, Richmond},
	month = aug,
	year = {2020},
	keywords = {Conducting research, read 100\%, read 40\%, read 60\%, read 80\%},
}

@article{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	urldate = {2020-09-23},
	journal = {arXiv:1703.03400 [cs]},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {arXiv: 1703.03400},
	keywords = {Meta-Learning},
}

@article{delgado_why_2019,
	title = {Why {Cohen}’s {Kappa} should be avoided as performance measure in classification},
	volume = {14},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6762152/},
	doi = {10.1371/journal.pone.0222916},
	abstract = {We show that Cohen’s Kappa and Matthews Correlation Coefficient (MCC), both extended and contrasted measures of performance in multi-class classification, are correlated in most situations, albeit can differ in others. Indeed, although in the symmetric case both match, we consider different unbalanced situations in which Kappa exhibits an undesired behaviour, i.e. a worse classifier gets higher Kappa score, differing qualitatively from that of MCC. The debate about the incoherence in the behaviour of Kappa revolves around the convenience, or not, of using a relative metric, which makes the interpretation of its values difficult. We extend these concerns by showing that its pitfalls can go even further. Through experimentation, we present a novel approach to this topic. We carry on a comprehensive study that identifies an scenario in which the contradictory behaviour among MCC and Kappa emerges. Specifically, we find out that when there is a decrease to zero of the entropy of the elements out of the diagonal of the confusion matrix associated to a classifier, the discrepancy between Kappa and MCC rise, pointing to an anomalous performance of the former. We believe that this finding disables Kappa to be used in general as a performance measure to compare classifiers.},
	number = {9},
	urldate = {2020-09-23},
	journal = {PLoS ONE},
	author = {Delgado, Rosario and Tibau, Xavier-Andoni},
	month = sep,
	year = {2019},
	pmid = {31557204},
	pmcid = {PMC6762152},
	keywords = {Evaluation metrics},
}

@article{atz_tau_nodate,
	title = {The {Tau} of {Data}: {A} {New} {Metric} to {Assess} the {Timeliness} of {Data} in {Catalogues}},
	abstract = {We review existing studies that assess the timeliness of data in catalogues and propose a new metric: tau, the percentage of datasets up-to-date in a data catalogue. Obsolete data will stifle innovation, whereas spotlighting timeliness can foster efficiency and support the sustainability of the open data ecosystem, for example, by encouraging automated publication of data.We validate the tau in three case studies: the World Bank catalogue, the UK data catalogue (data.gov.uk) and the London Datastore. For the World Bank and London we find that roughly half of the datasets are up-to-date, whereas data.gov.uk performs worse.},
	language = {en},
	author = {Atz, Ulrich},
	keywords = {Evaluation metrics},
	pages = {11},
}
